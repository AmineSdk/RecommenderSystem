{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CF_Autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-text"
      ],
      "metadata": {
        "id": "nFsPPT0-cZms",
        "outputId": "55987cd1-618f-474e-8b5d-5c487011a8bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 511.7 MB 6.2 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.10.0,>=2.9.0rc0\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 49.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.15.0)\n",
            "Collecting flatbuffers<2,>=1.12\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0\n",
            "  Downloading keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 55.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.14.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.1.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (4.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.47.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.26.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (21.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (57.4.0)\n",
            "Collecting tensorboard<2.10,>=2.9\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 43.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (14.0.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (1.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.10,>=2.9.0->tensorflow-text) (3.1.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.23.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.4.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.6.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.35.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<2.10,>=2.9.0->tensorflow-text) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, flatbuffers, tensorflow, tensorflow-text\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: flatbuffers\n",
            "    Found existing installation: flatbuffers 2.0\n",
            "    Uninstalling flatbuffers-2.0:\n",
            "      Successfully uninstalled flatbuffers-2.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed flatbuffers-1.12 gast-0.4.0 keras-2.9.0 tensorboard-2.9.1 tensorflow-2.9.1 tensorflow-estimator-2.9.0 tensorflow-text-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d aminesedkaoui/epinions-ratings-500k"
      ],
      "metadata": {
        "id": "UM0vifCk9CB0",
        "outputId": "265fa9b6-bcc6-4f95-86d6-73239869744d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading epinions-ratings-500k.zip to /content\n",
            "  0% 0.00/3.25M [00:00<?, ?B/s]\n",
            "100% 3.25M/3.25M [00:00<00:00, 218MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TcGcg4ZzRQD1"
      },
      "outputs": [],
      "source": [
        "from scipy.sparse import csr_matrix\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import pandas as pd\n",
        "import random\n",
        "from keras import datasets, layers, models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/epinions-ratings-500k.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "ppVGlIh8Ce8q",
        "outputId": "6494443b-268e-4716-cb35-1a7ebbd0bf23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/epinions-ratings-500k.zip\n",
            "  inflating: /content/epinions_rating_500k.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/epinions_rating_500k.csv')\n",
        "df = df.drop(['Unnamed: 0'],axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "M-Fxg9TcRStc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DatasetToUserItemDataFrame(dataframe,userID,itemID,rating):\n",
        "  #Setting new item IDs from string to int \n",
        "  itemKeys = [] \n",
        "  i = 0\n",
        "  for item in dataframe[itemID].value_counts(sort=False):\n",
        "    temp = np.full((item),i)\n",
        "    itemKeys = np.append(itemKeys,temp)\n",
        "    i += 1\n",
        "\n",
        "  #Setting new user IDs from string to int\n",
        "  userKeysDic = {}\n",
        "  userKeys = np.zeros((dataframe[userID].size))\n",
        "  i = 0\n",
        "  for user in dataframe[userID].unique():\n",
        "    userKeysDic[user] = i\n",
        "    i += 1\n",
        "  i = 0\n",
        "  for user in dataframe[userID]:\n",
        "    userKeys[i] = userKeysDic[user]\n",
        "    i += 1\n",
        "\n",
        "  #Converting arrays from float to int \n",
        "  userKeys = userKeys.astype(int)\n",
        "  itemKeys = itemKeys.astype(int)\n",
        "\n",
        "  \n",
        "  user_item = csr_matrix((dataframe[rating].values.astype(int),(userKeys,itemKeys))) #Creating sparse matrix\n",
        "  user_item_matrix = user_item.toarray() #Converting sparse matrix into array\n",
        "  df_user_item = pd.DataFrame(user_item_matrix,index = dataframe[userID].unique()  ,columns = dataframe[itemID].unique() ) \n",
        "\n",
        "  return user_item,df_user_item"
      ],
      "metadata": {
        "id": "HHmzbMvDVB0b"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_df(df):\n",
        "  users = df['User_ID'].unique()\n",
        "  items = df['Item_ID'].unique()\n",
        "  df_train = df.copy()\n",
        "\n",
        "  New_User_IDs = random.sample(range(10000,50000),df['User_ID'].nunique())\n",
        "  New_Item_IDs = random.sample(range(100000,200000),df['Item_ID'].nunique())\n",
        "  i = 0\n",
        "  for d in users :\n",
        "    df_train['User_ID'].replace({d : New_User_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "\n",
        "  i = 0\n",
        "  for d in items :\n",
        "    df_train['Item_ID'].replace({d : New_Item_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "  \n",
        "  df_train['one']=df_train['rating'].apply(lambda x: 1 if x==1 else 0)\n",
        "  df_train['two']=df_train['rating'].apply(lambda x: 1 if x==2 else 0)\n",
        "  df_train['three']=df_train['rating'].apply(lambda x: 1 if x==3 else 0)\n",
        "  df_train['four']=df_train['rating'].apply(lambda x: 1 if x==4 else 0)\n",
        "  df_train['five']=df_train['rating'].apply(lambda x: 1 if x==5 else 0)\n",
        "  df_train['six']=df_train['rating'].apply(lambda x: 1 if x==6 else 0)\n",
        "\n",
        "  return df_train"
      ],
      "metadata": {
        "id": "08ifZdzgO2u9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GMF\n",
        "\n",
        "def Create_GMF(num_users,num_items,SIZE_):\n",
        "\n",
        "  input_userID = layers.Input(shape=[1], name='user_ID')\n",
        "  input_itemID = layers.Input(shape=[1], name='item_ID')\n",
        "\n",
        "  user_emb_GMF = layers.Embedding(num_users, SIZE_, name='user_emb_GMF')(input_userID)\n",
        "  item_emb_GMF = layers.Embedding(num_items, SIZE_, name='item_emb_GMF')(input_itemID)\n",
        "\n",
        "  u_GMF = layers.Flatten()(user_emb_GMF)\n",
        "  i_GMF = layers.Flatten()(item_emb_GMF)\n",
        "\n",
        "  dot_layer = layers.Multiply()([u_GMF, i_GMF])\n",
        "\n",
        "  out_layer = layers.Dense(6, activation='softmax', name='output')(dot_layer)\n",
        "\n",
        "  GMF = tf.keras.Model([input_userID, input_itemID], out_layer)\n",
        "  \n",
        "  return GMF\n",
        "\n",
        "def Train_GMF(model,X_train,Y_train,nbrEpochs):\n",
        "  model.compile(optimizer = 'adam',\n",
        "                    loss = tf.keras.losses.categorical_crossentropy ,\n",
        "                    metrics=['accuracy'])\n",
        "  model.fit(X_train,Y_train,epochs = nbrEpochs)\n",
        "  return model\n",
        "\n",
        "def Fill_Cf_Matrix(model,dataframe,userIDs,itemIDs):\n",
        "  \n",
        "  for user in userIDs:\n",
        "    user_s = pd.Series(user)\n",
        "    for item in itemIDs:\n",
        "      item_s = pd.Series(item)\n",
        "      if dataframe.loc[user][item] == 0:\n",
        "        prediction = model.predict([user_s,item_s],verbose = 0)\n",
        "        result = np.where(prediction[0] == np.amax(prediction[0]))[0][0] + 1\n",
        "        dataframe.loc[user][item] = result\n",
        "  return dataframe\n"
      ],
      "metadata": {
        "id": "z6KHUFF9gwT5"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = preprocess_df(df)"
      ],
      "metadata": {
        "id": "5k7vteutejcS"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GMF = Create_GMF(df_train['User_ID'].max() + 1,df_train['Item_ID'].max() + 1,64)"
      ],
      "metadata": {
        "id": "HHrM6Dgl8AP1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "df_train_sample = df_train[:250]\n",
        "X_train, X_test,y_train,y_test= train_test_split(df_train_sample[['User_ID','Item_ID']],df_train_sample[['one','two','three','four','five','six']],stratify=df_train_sample[['one','two','three','four','five','six']])\n",
        "#X_train, X_test,y_train,y_test= train_test_split(df_train[['User_ID','Item_ID']],df_train['rating'],stratify=df_train['rating'])"
      ],
      "metadata": {
        "id": "BFid5Xf9_SG8"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GMF_trained = Train_GMF(GMF,[X_train['User_ID'],X_train['Item_ID']],y_train,1)"
      ],
      "metadata": {
        "id": "grxA3dxt-HSR",
        "outputId": "18c0af0c-40a5-486b-9482-914937454661",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 1s 159ms/step - loss: 1.7178 - accuracy: 0.7701\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_item_matrix , df_mat = DatasetToUserItemDataFrame(df_train_sample,'User_ID','Item_ID','rating')"
      ],
      "metadata": {
        "id": "Y5_US3dQs3ro"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GMF_trained.evaluate([X_test['User_ID'],X_test['Item_ID']],y_test)"
      ],
      "metadata": {
        "id": "AXTEr-r2tqij",
        "outputId": "de51e249-6fc0-4820-ee73-f2211476fb45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 6ms/step - loss: 1.7126 - accuracy: 0.7778\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.7125930786132812, 0.7777777910232544]"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = GMF_trained.predict([X_test['User_ID'],X_test['Item_ID']])\n",
        "result = np.where(pred[0] == np.amax(pred[0]))\n",
        "result[0][0]"
      ],
      "metadata": {
        "id": "fnagT7ycdBv7",
        "outputId": "73a9d507-d882-49e4-b71e-0977e53e45ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 31ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "New_User_IDs = df_train_sample.User_ID.unique()\n",
        "New_Item_IDs = df_train_sample.Item_ID.unique()"
      ],
      "metadata": {
        "id": "1f_e0Smif07B"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled = Fill_Cf_Matrix(GMF_trained,df_mat,New_User_IDs,New_Item_IDs)"
      ],
      "metadata": {
        "id": "oqWfFRyBr0mJ"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat"
      ],
      "metadata": {
        "id": "rGSQo1SV_-iD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Autoencoder\n",
        "\n",
        "encoder_input = layers.Input(shape=(X_train.shape[1]),name='user_item')\n",
        "flat = layers.Flatten()(encoder_input)\n",
        "hid_encoder = layers.Dense(512,activation=\"relu\")(flat)\n",
        "encoder_output = layers.Dense(256,activation=\"relu\")(hid_encoder)\n",
        "\n",
        "decoder_input = layers.Dense(512,activation=\"relu\")(encoder_output)\n",
        "decoder_output = layers.Dense(X_train.shape[1],activation=\"relu\")(decoder_input)\n",
        "\n",
        "autoencoder = tf.keras.Model(inputs = encoder_input, outputs = decoder_output)"
      ],
      "metadata": {
        "id": "YFYfUYa-e3w9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.compile(optimizer = 'adam',\n",
        "                    loss = 'mae',\n",
        "                    metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "_h1-_AumilKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.summary()"
      ],
      "metadata": {
        "id": "QxDl7Q-1jPdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder.fit(X_train, X_train, epochs = 5)"
      ],
      "metadata": {
        "id": "dh7hlrNijRGN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}