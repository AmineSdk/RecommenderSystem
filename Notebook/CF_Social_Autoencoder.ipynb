{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CF_Social_Autoencoder.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmineSdk/RecommenderSystem/blob/main/Notebook/CF_Social_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "4fRXByYkGvia",
        "outputId": "b719ebfc-6aea-4fbf-fc00-618f9d00125f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle datasets download yelp-shopping-rating-review -f review_shopping.csv\n",
        "# ! kaggle datasets download Yelp-shopping-GMF-filled-mat -f Yelp_shopping_GMF_filled_mat.csv\n",
        "! kaggle datasets download yelp-restaurants-review-rating -f review_restaurant.csv\n",
        "#! kaggle datasets download Aamazon-Cellphones-35k-GMF-filled-mat -f Aamazon_Cellphones_35k_GMF_filled_mat.csv\n",
        "#! kaggle datasets download Yelp-restaurants-GMF-filled-mat -f Yelp_restaurants_GMF_filled_mat.csv"
      ],
      "metadata": {
        "id": "UM0vifCk9CB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-ranking"
      ],
      "metadata": {
        "id": "KGV_L6WsJ6gv",
        "outputId": "d67ec10a-1fc1-477a-9425-1cd9a8f2505e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-ranking\n",
            "  Downloading tensorflow_ranking-0.5.0-py2.py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 13.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (1.2.0)\n",
            "Collecting tensorflow-serving-api<3.0.0,>=2.0.0\n",
            "  Downloading tensorflow_serving_api-2.10.0-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow-ranking) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.17.3)\n",
            "Requirement already satisfied: grpcio<2,>=1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.47.0)\n",
            "Collecting tensorflow<3,>=2.10.0\n",
            "  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 16 kB/s \n",
            "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (21.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.14.1)\n",
            "Collecting keras<2.11,>=2.10.0\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 51.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (14.0.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (57.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.1.2)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.1.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.26.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.1.0)\n",
            "Collecting tensorboard<2.11,>=2.10\n",
            "  Downloading tensorboard-2.10.0-py3-none-any.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 57.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 71.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.5.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.4.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.23.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow<3,>=2.10.0->tensorflow-serving-api<3.0.0,>=2.0.0->tensorflow-ranking) (3.0.9)\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras, gast, tensorflow, tensorflow-serving-api, tensorflow-ranking\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.8.0\n",
            "    Uninstalling tensorflow-estimator-2.8.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.8.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.8.0\n",
            "    Uninstalling tensorboard-2.8.0:\n",
            "      Successfully uninstalled tensorboard-2.8.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.8.0\n",
            "    Uninstalling keras-2.8.0:\n",
            "      Successfully uninstalled keras-2.8.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.5.3\n",
            "    Uninstalling gast-0.5.3:\n",
            "      Successfully uninstalled gast-0.5.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n",
            "    Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n",
            "      Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n",
            "Successfully installed gast-0.4.0 keras-2.10.0 tensorboard-2.10.0 tensorflow-2.10.0 tensorflow-estimator-2.10.0 tensorflow-ranking-0.5.0 tensorflow-serving-api-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TcGcg4ZzRQD1"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from scipy.sparse import csr_matrix\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import random\n",
        "from keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from progressbar import progressbar\n",
        "from sklearn.compose import ColumnTransformer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def amazonPreprocess(df):\n",
        "  \n",
        "  df = df[:35000]\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.rename(columns = {'reviewerID':'userID', 'productID' : 'itemID'},inplace = True)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  print(\"DATASET PREPROCESSED\")\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "lAZGdMRODRqE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yelpPreprocess(df):\n",
        "\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.drop(columns=['funny', 'cool','review_id','useful','date'],inplace=True)\n",
        "  df.rename(columns = {'user_id':'userID', 'business_id':'itemID','stars':'rating'},inplace = True)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  print(\"DATASET PREPROCESSED\")\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "03zm3UHNDu8I"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DatasetToUserItemDataFrame(dataframe,userID,itemID,rating):\n",
        "  #Setting new item IDs from string to int \n",
        "  itemKeys = [] \n",
        "  i = 0\n",
        "  for item in dataframe[itemID].value_counts(sort=False):\n",
        "    temp = np.full((item),i)\n",
        "    itemKeys = np.append(itemKeys,temp)\n",
        "    i += 1\n",
        "\n",
        "  #Setting new user IDs from string to int\n",
        "  userKeysDic = {}\n",
        "  userKeys = np.zeros((dataframe[userID].size))\n",
        "  i = 0\n",
        "  for user in dataframe[userID].unique():\n",
        "    userKeysDic[user] = i\n",
        "    i += 1\n",
        "  i = 0\n",
        "  for user in dataframe[userID]:\n",
        "    userKeys[i] = userKeysDic[user]\n",
        "    i += 1\n",
        "\n",
        "  #Converting arrays from float to int \n",
        "  userKeys = userKeys.astype(int)\n",
        "  itemKeys = itemKeys.astype(int)\n",
        "  print(userKeys.size)\n",
        "  print(itemKeys.size)\n",
        "  \n",
        "  user_item = csr_matrix((dataframe[rating].values,(userKeys,itemKeys))) #Creating sparse matrix\n",
        "  user_item_matrix = user_item.toarray() #Converting sparse matrix into array\n",
        "  df_user_item = pd.DataFrame(user_item_matrix,index = dataframe[userID].unique()  ,columns = dataframe[itemID].unique() ) \n",
        "\n",
        "  return df_user_item\n",
        "\n",
        "def getUsersRatings(df,usersList): #returns df_rating containing only users that have friends \n",
        "  temp = df.copy()\n",
        "  for user in list(df['userID']):\n",
        "    if user not in usersList:\n",
        "      temp = temp.drop(temp.loc[temp['userID'] == user].index)\n",
        "  return temp\n",
        "\n",
        "def genIndexColumn(df,min_index,max_index,min_column,max_column): #generates random int index and columns for a given df\n",
        "  New_User_IDs = random.sample(range(min_index,max_index),df.index.size)\n",
        "  New_Item_IDs = random.sample(range(min_column,max_column),df.columns.size)\n",
        "\n",
        "  df_new = pd.DataFrame(df.to_numpy(),index = New_User_IDs,columns = New_Item_IDs)\n",
        "  return df_new\n",
        "\n",
        "def generateIDs(df,index,columns,min_index,max_index,min_column,max_column):\n",
        "  users = df[index].unique()\n",
        "  items = df[columns].unique()\n",
        "  df_train = df.copy()\n",
        "\n",
        "  New_User_IDs = random.sample(range(min_index,max_index),df[index].nunique())\n",
        "  New_Item_IDs = random.sample(range(min_column,max_column),df[columns].nunique())\n",
        "  i = 0\n",
        "  for d in progressbar(users) :\n",
        "    df_train[index].replace({d : New_User_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "\n",
        "  i = 0\n",
        "  for d in progressbar(items) :\n",
        "    df_train[columns].replace({d : New_Item_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "  return df_train\n",
        "\n",
        "def processGmfData(df,index,column,min_user_id,max_user_id,min_item_id,max_item_id):\n",
        "  \n",
        "  \n",
        "  df.rename(columns = {'text': 'reviewText', 'stars': 'rating', 'business_id': 'itemID', 'user_id': 'userID'}, inplace = True)\n",
        "  #df.rename(columns = {'reviewerID':'userID', 'productID' : 'itemID'},inplace = True)\n",
        "  df = generateIDs(df,index,column,min_user_id,max_user_id,min_item_id,max_item_id)\n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "\n",
        "  df['one'] = df['rating'].apply(lambda x: 1 if x==1 else 0)\n",
        "  df['two'] = df['rating'].apply(lambda x: 1 if x==2 else 0)\n",
        "  df['three'] = df['rating'].apply(lambda x: 1 if x==3 else 0)\n",
        "  df['four'] = df['rating'].apply(lambda x: 1 if x==4 else 0)\n",
        "  df['five'] = df['rating'].apply(lambda x: 1 if x==5 else 0)\n",
        "  #df['six'] = df['rating'].apply(lambda x: 1 if x==6 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "08ifZdzgO2u9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GMF\n",
        "def getGMFmodel(num_users,num_items,SIZE_):\n",
        "\n",
        "  input_userID = layers.Input(shape=[1], name='user_ID')\n",
        "  input_itemID = layers.Input(shape=[1], name='item_ID')\n",
        "\n",
        "  user_emb_GMF = layers.Embedding(num_users, SIZE_, name='user_emb_GMF')(input_userID)\n",
        "  item_emb_GMF = layers.Embedding(num_items, SIZE_, name='item_emb_GMF')(input_itemID)\n",
        "\n",
        "  u_GMF = layers.Flatten()(user_emb_GMF)\n",
        "  i_GMF = layers.Flatten()(item_emb_GMF)\n",
        "\n",
        "  dot_layer = layers.Multiply()([u_GMF, i_GMF])\n",
        "\n",
        "  out_layer = layers.Dense(1, activation='sigmoid', name='output')(dot_layer)\n",
        "\n",
        "  GMF = tf.keras.Model([input_userID, input_itemID], out_layer)\n",
        "  \n",
        "  return GMF\n",
        "\n",
        "\n",
        "def user_item_ID_lists(userIDs,itemIDs):\n",
        "  i = 0\n",
        "  item_s = pd.Series()\n",
        "  user_s = pd.Series()\n",
        "\n",
        "  if userIDs.size > itemIDs.size:\n",
        "    for item in progressbar(itemIDs):\n",
        "      temp = pd.Series(userIDs)\n",
        "      user_s = user_s.append(temp)\n",
        "      temp = []\n",
        "      temp = [item for user in userIDs]\n",
        "      temp = pd.Series(temp)\n",
        "      item_s = item_s.append(temp)\n",
        "  else:\n",
        "    for user in progressbar(userIDs):\n",
        "      temp = pd.Series(itemIDs)\n",
        "      item_s = item_s.append(temp)\n",
        "      temp = []\n",
        "      temp = [user for item in itemIDs]\n",
        "      temp = pd.Series(temp)\n",
        "      user_s = user_s.append(temp)\n",
        "\n",
        "  return user_s,item_s\n",
        "\n",
        "\n",
        "def Fill_Cf_Matrix(model,userList,itemList,userIDs,itemIDs):\n",
        "  rowLen = userIDs.size if userIDs.size > itemIDs.size else itemIDs.size\n",
        "\n",
        "  prediction = model.predict([userList,itemList],verbose = 1)\n",
        "  i = 0\n",
        "  row = []\n",
        "  matrix = []\n",
        "  print(\"pred done\")\n",
        "  \n",
        "  for i in progressbar(range(userList.shape[0])):\n",
        "    #result = np.where(prediction[i] == np.amax(prediction[i]))[0][0] + 1\n",
        "    result = prediction[i][0]\n",
        "    row.append(result)\n",
        "    if len(row) == rowLen:\n",
        "      matrix.append(row)\n",
        "      row = []\n",
        "       \n",
        "    i += 1\n",
        "\n",
        "  matrix_arr = np.array(matrix)\n",
        "  if(userIDs.size > itemIDs.size):\n",
        "    matrix_arr = matrix_arr.transpose()\n",
        "  dataframe = pd.DataFrame(matrix_arr, index = userIDs, columns = itemIDs)\n",
        "  \n",
        "  return dataframe\n",
        "\n",
        "def loadDataset(fileID):\n",
        "  dataset = pd.read_csv(DATA[fileID])\n",
        "  print(\"== FILE LOADED ==\")\n",
        "  return dataset\n",
        "\n",
        "def saveDataframe(df,fileID):\n",
        "  file_path = PATH+'GMF_filled_'+DATA[fileID]\n",
        "  df.to_csv(file_path)\n",
        "\n",
        "def genGMFmat(df,df_test,modelID,fileID,nbrE,lossF,OF,emb,filter=None,cb=None):\n",
        "\n",
        "  # df_og = loadDataset(fileID)\n",
        "  # df = processGmfData(df_og,'userID','itemID',1000,2000+df_og['reviewerID'].nunique(),60000,61000+df_og['productID'].nunique())\n",
        "  \n",
        "  x_train ,x_test,y_train,y_test = train_test_split(df_test[['userID','itemID']],df_test['rating'],test_size=0.2)\n",
        "  model_trained = trainModel(modelID,nbrE,lossF,OF,[x_train['userID'],x_train['itemID']],y_train,maxUserID=df_test['userID'].max() + 1,maxItemID =df_test['itemID'].max() + 1,embed_size=emb)\n",
        "  model_trained.evaluate([x_test['userID'],x_test['itemID']],y_test)\n",
        "\n",
        "  #df_mat_filled = FillSparseMat(model_trained,df_test,df)\n",
        "  \n",
        "  return model_trained\n",
        "\n",
        "def FillSparseMat(model,df_prepro,df_og):\n",
        "\n",
        "  user_s,item_s = user_item_ID_lists(df_prepro['userID'].unique(),df_prepro['itemID'].unique())\n",
        "  #df_mat_filled = Fill_Cf_Matrix(model,user_s,item_s,df_og['userID'].unique(),df_og['itemID'].unique())\n",
        "\n",
        "  return user_s,item_s\n",
        "\n",
        "\n",
        "def trainModel(modelID,nbrEpochs,lossF,OF,x_train ,y_train ,mid_layer_ratio=None,nb_layers=None,maxUserID = None,maxItemID = None,embed_size = None,filter_size = None,bs = 16):\n",
        "  \n",
        "  if modelID =='BLCNN':\n",
        "    model = getBLCNNmodel(embed_size,filter_size)\n",
        "    \n",
        "  if modelID == 'GMF':\n",
        "    model = getGMFmodel(maxUserID,maxItemID,embed_size)\n",
        "    print(model.summary())\n",
        "  elif modelID == 'S-AutoCF':\n",
        "    model = getAutoCFmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "    print(model.summary())\n",
        "  elif modelID == 'SS-AutoCF':\n",
        "    model = getSS_HAEmodel(x_train[0],x_train[1],mid_layer_ratio,nb_layers)\n",
        "\n",
        "  model.compile(optimizer = OF,\n",
        "                    loss = lossF,\n",
        "                    metrics= METRICS)\n",
        "  model.fit(x_train,y_train,epochs = nbrEpochs,batch_size = bs,validation_split=0.14)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def getAutoCFmodel(x_train,mid_layer_ratio,nb_layers):\n",
        "  #mid_layer_ratio [0 - 1]\n",
        "  layer_ratio =  mid_layer_ratio + nb_layers*0.05\n",
        "\n",
        "  encoder_input = layers.Input(shape=(x_train.shape[1]),name='user_item')\n",
        "  flat = layers.Flatten()(encoder_input)\n",
        "\n",
        "  if nb_layers != 1:\n",
        "    hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"sigmoid\")(flat)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio -= 0.05\n",
        "      hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"sigmoid\")(hid_encoder)\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*x_train.shape[1],activation=\"sigmoid\")(hid_encoder)\n",
        "  else:\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*x_train.shape[1],activation=\"sigmoid\")(flat)\n",
        "\n",
        "  if nb_layers != 1:\n",
        "    decoder_input = layers.Dense(layer_ratio*x_train.shape[1],activation=\"sigmoid\")(encoder_output)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio += 0.05\n",
        "      decoder_input = layers.Dense(layer_ratio*x_train.shape[1],activation=\"sigmoid\")(decoder_input)\n",
        "    decoder_output = layers.Dense(x_train.shape[1],activation=\"sigmoid\")(decoder_input)\n",
        "  else:\n",
        "    decoder_output = layers.Dense(x_train.shape[1],activation=\"sigmoid\")(encoder_output)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs = encoder_input, outputs = decoder_output)\n",
        "\n",
        "  return autoencoder\n",
        "\n",
        "def getSS_HAEmodel(x_train_rat,x_train_trust,mid_layer_ratio,nb_layers):\n",
        "  \n",
        "  #mid_layer_ratio [0 - 1]\n",
        "  layer_ratio =  mid_layer_ratio + nb_layers*0.1\n",
        "\n",
        "  #Social_Autoencoder\n",
        "  #layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]\n",
        "  rating_input = layers.Input(shape=(x_train_rat.shape[1]),name='user_item')\n",
        "  social_input = layers.Input(shape=(x_train_trust.shape[1]),name='user_user')\n",
        "\n",
        "  flat_rating = layers.Flatten()(rating_input)\n",
        "  flat_social = layers.Flatten()(social_input)\n",
        "\n",
        "  SharedLayer_encoder = layers.Concatenate()([flat_rating,flat_social])\n",
        "\n",
        "  if nb_layers !=1:\n",
        "    hid_encoder = layers.Dense(layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(SharedLayer_encoder)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio -= 0.1\n",
        "      hid_encoder = layers.Dense(layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(hid_encoder)\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(hid_encoder)\n",
        "  else:\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(SharedLayer_encoder)\n",
        "  if nb_layers !=1:\n",
        "    hid_decoder = layers.Dense(layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(encoder_output)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio += 0.1\n",
        "      hid_decoder = layers.Dense(layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(hid_decoder)\n",
        "    SharedLayer_decoder =  layers.Dense(x_train_rat.shape[1]+x_train_trust.shape[1],activation=\"sigmoid\")(hid_decoder)\n",
        "  else:\n",
        "    SharedLayer_decoder =  layers.Dense(x_train_rat.shape[1]+x_train_trust.shape[1],activation=\"sigmoid\")(encoder_output)\n",
        "\n",
        "  rating_decoded , social_decoded = tf.split(SharedLayer_decoder,[x_train_rat.shape[1],x_train_trust.shape[1]],1)\n",
        "\n",
        "  # rating_output = layers.Dense(x_train_rat.shape[1],activation=\"relu\",name='rating_output')(SharedLayer_decoder)\n",
        "  # social_output = layers.Dense(x_train_trust.shape[1],activation=\"relu\",name='social_output')(SharedLayer_decoder)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs = [rating_input,social_input], outputs = [rating_decoded,social_decoded])\n",
        "\n",
        "  return autoencoder\n",
        "\n",
        "def createAutoCF(modelID,input_fileID,target_fileID,nbrEpochs,lossF,OF,mid_layer_ratio,nb_layers,bs):\n",
        "\n",
        "  sparseDf = loadDataset(input_fileID)  \n",
        "  sparseDf = yelpPreprocess(sparseDf)\n",
        "  # sparseDf.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  df_mat_filled = loadDataset(target_fileID)\n",
        "  df_mat_filled = df_mat_filled.set_index('Unnamed: 0')\n",
        "  df_mat = DatasetToUserItemDataFrame(sparseDf,'userID','itemID','rating')\n",
        "  # if df_mat.shape[0]< df_mat.shape[1]:\n",
        "  #   df_mat = df_mat.T\n",
        "  #   df_mat_filled = df_mat_filled.T\n",
        "  #   print(df_mat_filled.shape,df_mat.shape)\n",
        "  x_train,x_test,y_train,y_test = train_test_split(df_mat,df_mat_filled,test_size = 0.2)\n",
        "  model_trained = trainModel(modelID,nbrEpochs,lossF,OF,x_train,y_train,mid_layer_ratio,nb_layers,bs = bs)\n",
        "\n",
        "  return model_trained , x_test, y_test\n",
        "\n",
        "def createSS_HAE(modelID,input_rating_fileID,target_rating_fileID,trust_fileID,nbrEpochs,lossF,OF,mid_layer_ratio,nb_layers,bs):\n",
        "\n",
        "  sparseDf = loadDataset(input_rating_fileID)\n",
        "  sparseDf = yelpPreprocess(sparseDf)\n",
        "  # sparseDf.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  df_rating_filled = loadDataset(target_rating_fileID)\n",
        "  df_rating_filled = df_rating_filled.set_index('Unnamed: 0')\n",
        "  df_trust_mat = loadDataset(trust_fileID)\n",
        "  df_trust_mat = df_trust_mat.set_index('Unnamed: 0.1')\n",
        "  df_trust_mat.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "  # trust_users_list = list(df_trust_mat.index)\n",
        "  # df_rating_filled = getUsersRatings(df_rating_filled,trust_users_list) \n",
        "  # df_rating_filled = orgDataframe(df_rating_filled,trust_users_list)\n",
        "\n",
        "  #input & target for autoencoder training\n",
        "  df_rating_mat = DatasetToUserItemDataFrame(sparseDf,'userID','itemID','rating')\n",
        "  # df_rating_mat_filled = orgMatDataframe(df_rating_mat_filled,trust_users_list)\n",
        "\n",
        "  #data split\n",
        "  x_rating_train,x_rating_test,y_rating_train,y_rating_test = train_test_split(df_rating_mat,df_rating_filled)\n",
        "  x_train,x_test = train_test_split(df_trust_mat)\n",
        "\n",
        "  model_trained = trainModel(modelID,nbrEpochs,lossF,OF,[x_rating_train,x_train],[y_rating_train,x_train],mid_layer_ratio,nb_layers,bs=bs)\n",
        "\n",
        "  return model_trained , [x_rating_test,y_rating_test] , x_test\n",
        "\n",
        "def evaluateModel(model,x_test,y_test):\n",
        "  model.evaluate(x_test,y_test) \n",
        "\n",
        "def orgDataframe(df_to_org,org_list):\n",
        "\n",
        "  df_org = pd.DataFrame(columns = ['userID','itemID','rating','reviewText'])\n",
        "  for user in org_list:\n",
        "    df_temp = df_to_org[(df_to_org['userID'] == user )]\n",
        "    df_org = pd.concat([df_org,df_temp])\n",
        "\n",
        "  return df_org\n",
        "\n",
        "def orgMatDataframe(df_to_org,org_list):\n",
        "  \n",
        "  df_org = pd.DataFrame(index = list(df_to_org.index),columns = list(df_to_org.columns))\n",
        "  for user in org_list:\n",
        "    df_org.loc[user] = list(df_to_org.loc[user])\n",
        "  \n",
        "  return df_org\n",
        "\n",
        "def getItemsScore(userID,modelID,fileID):\n",
        "  \n",
        "  listeItemScore = pd.DataFrame()\n",
        "  \n",
        "  if modelID == 0:\n",
        "    listItemScore = predSeCF(userID,modelID,fileID)   #CF\n",
        "  elif modelID == 1:\n",
        "    listItemScore = predSeCF(userID,modelID,fileID)   #CF + Sentiment\n",
        "  elif modelID == 2:\n",
        "    listItemScore = predSSeCF(userID,modelID,fileID)  #CF + Social\n",
        "  elif modelID == 3:\n",
        "    listItemScore = predSSeCF(userID,modelID,fileID)  #CF + Sentiment + Social\n",
        "\n",
        "  return listeItemScore \n",
        "\n",
        "def getModel(modelID):\n",
        "  model = tf.keras.models.load_model(MODEL_PATH[modelID])\n",
        "  return model\n",
        "\n",
        "def loadDataFrame(df):\n",
        "  df_loaded = pd.read_csv(DATA[df])\n",
        "  return  df_loaded\n",
        "\n",
        "def predSeCF(userID,modelID,fileID):\n",
        "  \n",
        "  model = getModel(modelID)\n",
        "  df_user_item_mat = pd.read_csv(DATA[fileID])\n",
        "  SparseScoresVec = df_user_item_mat.loc[userID].to_numpy()\n",
        "  listItemScores = df_user_item_mat.loc[userID].to_frame()\n",
        "  listItemScores = listItemScores.reset_index()\n",
        "  listItemScores.set_axis(['itemID','score'],axis='columns',inplace=True)\n",
        "  PredScoresVec = model.predict(SparseScoresVec)\n",
        "  listItemScores.replace(SparseScoresVec,PredScoresVec,inplace = True)\n",
        "\n",
        "  return listItemScores\n",
        "\n",
        "def predSSeCF(userID,modelID,ratings_fileID,trust_fileID):\n",
        "\n",
        "  model = getModel(modelID)\n",
        "  df_user_item_mat = loadDataset(fileID)\n",
        "  df_user_user_mat = pd.read_csv(trust_fileID)\n",
        "  SparseScoresVec = df_user_item_mat.loc[userID].to_numpy()\n",
        "  SparseTrustVec = df_user_user_mat.loc[userID].to_numpy()\n",
        "  listItemScores = df_user_item_mat.loc[userID].to_frame()\n",
        "  listItemScores = listItemScores.reset_index()\n",
        "  listItemScores.set_axis(['itemID','score'],axis='columns',inplace=True)\n",
        "  PredScoresVec,SparseTrustVec = model.predict([SparseScoresVec,SparseTrustVec])\n",
        "  listItemScores.replace(SparseScoresVec,PredScoresVec,inplace = True)\n",
        "\n",
        "  return listItemScores\n",
        "  "
      ],
      "metadata": {
        "id": "z6KHUFF9gwT5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datapreprocess(df_sample):\n",
        "  \n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "\n",
        "  df_sample = generateIDs(df_sample,'userID','itemID',5000,6000+df_sample['userID'].nunique(),8000,8000+df_sample['itemID'].nunique())\n",
        "  \n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df_sample"
      ],
      "metadata": {
        "id": "EHC86cO8GJcK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(df):\n",
        "  transformer = ColumnTransformer(transformers=[('norm', preprocessing.MinMaxScaler(feature_range=(0, 1)), ['stars'])])\n",
        "  col = transformer.fit_transform(df)\n",
        "  return col"
      ],
      "metadata": {
        "id": "SPUgPW_LMXYI"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_ranking as tfr"
      ],
      "metadata": {
        "id": "TZ7BFJ0HlG0P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = ['/content/gdrive/MyDrive/yelp SHOP/shopping_Beta_0.7_norm.csv','/content/gdrive/MyDrive/yelp SHOP/Yelp_shopping_GMF_filled_mat_0.7.csv','/content/gdrive/MyDrive/yelp SHOP/SHOPPING_Final_UserUser_trust_DF.csv','/content/Yelp_shopping_GMF_filled_mat.csv']\n",
        "MODEL_PATH = []\n",
        "PATH = ''\n",
        "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "precision = tf.keras.metrics.Precision()\n",
        "# ndcg = tfr.keras.metrics.NDCGMetric()\n",
        "METRICS = ['mae',rmse]"
      ],
      "metadata": {
        "id": "3eS1ZsrzFU-m"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cbf_rest = pd.read_csv('/content/gdrive/MyDrive/yelp SHOP/NEW_SHOPPING_CBF_MATRIX.csv')"
      ],
      "metadata": {
        "id": "BUPRwF2FBGWZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cbf_shop = pd.read_csv('/content/gdrive/MyDrive/yelp SHOP/NEW_SHOPPING_CBF_MATRIX.csv')"
      ],
      "metadata": {
        "id": "1BK9lN6u0nRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rest =  pd.read_csv('/content/gdrive/MyDrive/yelp SHOP/shopping_Beta_0.7_norm.csv')"
      ],
      "metadata": {
        "id": "1YfCIATS0Ye2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cbf_rest.head()"
      ],
      "metadata": {
        "id": "BwKEMFj-2Tv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cbf_rest.drop(columns=['Unnamed: 0'],inplace=True)"
      ],
      "metadata": {
        "id": "9JUxjqWf0cHF"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cbf_rest = df_cbf_rest.set_index(['Unnamed: 0.1'])"
      ],
      "metadata": {
        "id": "hLNUfIpu05VK"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rest = yelpPreprocess(df_rest)"
      ],
      "metadata": {
        "id": "1eTKqeCK08Hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rest_mat = DatasetToUserItemDataFrame(df_rest,'userID','itemID','rating')"
      ],
      "metadata": {
        "id": "-FfVb2Gy0-x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_rest_mat.head()"
      ],
      "metadata": {
        "id": "mt3GR6Rk1Dw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test = train_test_split(df_rest_mat,test_size=0.2,shuffle=False)"
      ],
      "metadata": {
        "id": "FxzBfzpV1Hpi"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.head()"
      ],
      "metadata": {
        "id": "oZSivOfS1JLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cbf_rest = df_cbf_rest[x_test.columns.tolist()]"
      ],
      "metadata": {
        "id": "V7YfZwD71Ss5"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cbf_rest.head()"
      ],
      "metadata": {
        "id": "UJ5N-UWF1c0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cbf_rest = new_cbf_rest.loc[x_test.index.tolist()]"
      ],
      "metadata": {
        "id": "F_4H0VDL1qhT"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_cbf_rest.to_csv('/content/gdrive/MyDrive/yelp SHOP/hyb_SHOPPING_CBF_MATRIX.csv',index=False)"
      ],
      "metadata": {
        "id": "In0CvG-813Lv"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col = normalize(df_)\n",
        "df_.iloc[:,4]=col"
      ],
      "metadata": {
        "id": "PPyiN2t9KYbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_.to_csv('/content/gdrive/MyDrive/yelp SHOP/shopping_Beta_0.7_norm.csv',index=False)"
      ],
      "metadata": {
        "id": "AYJ1kr9eKrpc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,x_test,y_test = createAutoCF('S-AutoCF',0,1,10,tf.keras.losses.MeanSquaredError(),'adam',0.2,1,8)"
      ],
      "metadata": {
        "id": "jWu3NfsAGS2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateModel(model,x_test,y_test)"
      ],
      "metadata": {
        "id": "BKfbmFoUI7yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,[x_rating_test,y_rating_test],y_test = createSS_HAE('SS-AutoCF',0,1,2,30,tf.keras.losses.MeanSquaredError(),'adam',0.2,2,8)"
      ],
      "metadata": {
        "id": "muCu7AN1tOAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateModel(model,[x_rating_test,y_test],[y_rating_test,y_test])"
      ],
      "metadata": {
        "id": "6Yu8xuuKnpD0",
        "outputId": "9ebb8c95-1e76-4826-dc07-844101352006",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23/23 [==============================] - 0s 11ms/step - loss: 0.0141 - tf.split_loss: 0.0073 - tf.split_1_loss: 0.0068 - tf.split_mae: 0.0631 - tf.split_root_mean_squared_error: 0.0855 - tf.split_1_mae: 0.0465 - tf.split_1_root_mean_squared_error: 0.0827\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/gdrive/MyDrive/models/shop_CF_AS_SOC\")"
      ],
      "metadata": {
        "id": "u0_xGTElIBK-"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df__ = pd.read_csv('/content/gdrive/MyDrive/yelp SHOP/Yelp_shopping_GMF_filled_mat.csv')"
      ],
      "metadata": {
        "id": "L-2I967wvxPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train , y_test = train_test_split(df__,test_size=0.2,shuffle=False)"
      ],
      "metadata": {
        "id": "c9jVp4zxwgEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_ = yelpPreprocess(df_)\n",
        "# df_ = DatasetToUserItemDataFrame(df_,'userID','itemID','rating')\n",
        "x_train , x_test = train_test_split(df_,test_size=0.2,shuffle=False)"
      ],
      "metadata": {
        "id": "qOZOPJNNtGgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_l = tf.keras.models.load_model('/content/gdrive/MyDrive/yelp SHOP/CF')"
      ],
      "metadata": {
        "id": "aDlvJI-krgnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = y_test.set_index(['Unnamed: 0'])"
      ],
      "metadata": {
        "id": "k2V4CvnougtU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_l.evaluate(x_test,y_test)"
      ],
      "metadata": {
        "id": "VjxrevWFuVf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_test,_ = model.predict([x_rating_test,y_test])"
      ],
      "metadata": {
        "id": "KKrE8ei_DtCu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_test = model.predict(x_test)"
      ],
      "metadata": {
        "id": "XOgHQeODrwjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb3c2bb1-6c93-4e80-c770-fde2e04aff29"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19/19 [==============================] - 0s 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_array = np.array(y_rating_test)\n",
        "# precision_10 = tf.keras.metrics.Precision(top_k =10)\n",
        "ndcg10 = tfr.keras.metrics.NDCGMetric(topn=10)\n",
        "ndcg10.update_state(y_test_array,predicted_test)\n",
        "# precision_10.update_state(y_test_array,predicted_test)\n",
        "ndcg10.result().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6RXLSSCyNSD",
        "outputId": "f0e06609-fa07-4791-8fcf-e24685837d9c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7004668"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = pd.read_csv(\"/content/gdrive/MyDrive/yelp SHOP/shopping_Beta_0.7.csv\")"
      ],
      "metadata": {
        "id": "YFf76MumHofs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = df_sample.dropna(how='any',axis=0)\n",
        "df_sample.drop(columns=['funny', 'cool','review_id','useful','date'],inplace=True)\n",
        "df_sample.rename(columns = {'user_id':'userID', 'business_id':'itemID','stars':'rating'},inplace = True)\n",
        "df_sample.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)"
      ],
      "metadata": {
        "id": "mvGZ7ZP-IGG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.head()"
      ],
      "metadata": {
        "id": "ZnJVhwlYIluq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.shape"
      ],
      "metadata": {
        "id": "80bBQ-Asj1Df",
        "outputId": "df1c27f9-95ae-49ce-9e20-f9ef7d20bd8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(57258, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = datapreprocess(df_sample)"
      ],
      "metadata": {
        "id": "c3PUFoU5GrhR",
        "outputId": "3c33211b-f2aa-48fe-8db6-869ee65ffdc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0% (23 of 2935) |                      | Elapsed Time: 0:00:00 ETA:   0:00:12"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== PREPROCESSING DATA ...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100% (2935 of 2935) |####################| Elapsed Time: 0:00:15 Time:  0:00:15\n",
            "100% (9637 of 9637) |####################| Elapsed Time: 0:01:31 Time:  0:01:31\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== DATA PREPROCESSED ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col = normalize(df_test)\n",
        "df_test.iloc[:,3]=col"
      ],
      "metadata": {
        "id": "Gp3nDEvwZAme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "44G4_w6pgRAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genGMFmat(df_sample,df_test,'GMF',0,4,tf.keras.losses.MeanSquaredError(),'adam',32)"
      ],
      "metadata": {
        "id": "fZ9Cm1zW_lDY",
        "outputId": "e6b1ad8d-a07a-4803-8927-b1ea96336c7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " user_ID (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " item_ID (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " user_emb_GMF (Embedding)       (None, 1, 32)        285888      ['user_ID[0][0]']                \n",
            "                                                                                                  \n",
            " item_emb_GMF (Embedding)       (None, 1, 32)        564384      ['item_ID[0][0]']                \n",
            "                                                                                                  \n",
            " flatten_12 (Flatten)           (None, 32)           0           ['user_emb_GMF[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_13 (Flatten)           (None, 32)           0           ['item_emb_GMF[0][0]']           \n",
            "                                                                                                  \n",
            " multiply_6 (Multiply)          (None, 32)           0           ['flatten_12[0][0]',             \n",
            "                                                                  'flatten_13[0][0]']             \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 1)            33          ['multiply_6[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 850,305\n",
            "Trainable params: 850,305\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/4\n",
            "2463/2463 [==============================] - 10s 4ms/step - loss: 0.0697 - mae: 0.2214 - root_mean_squared_error: 0.2633 - val_loss: 0.0611 - val_mae: 0.1946 - val_root_mean_squared_error: 0.2471\n",
            "Epoch 2/4\n",
            "2463/2463 [==============================] - 18s 7ms/step - loss: 0.0527 - mae: 0.1783 - root_mean_squared_error: 0.2296 - val_loss: 0.0591 - val_mae: 0.1930 - val_root_mean_squared_error: 0.2432\n",
            "Epoch 3/4\n",
            "2463/2463 [==============================] - 11s 4ms/step - loss: 0.0208 - mae: 0.1048 - root_mean_squared_error: 0.1443 - val_loss: 0.0629 - val_mae: 0.1977 - val_root_mean_squared_error: 0.2509\n",
            "Epoch 4/4\n",
            "2463/2463 [==============================] - 10s 4ms/step - loss: 0.0058 - mae: 0.0547 - root_mean_squared_error: 0.0762 - val_loss: 0.0648 - val_mae: 0.1994 - val_root_mean_squared_error: 0.2545\n",
            "358/358 [==============================] - 1s 2ms/step - loss: 0.0642 - mae: 0.1995 - root_mean_squared_error: 0.2534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_s,item_s = FillSparseMat(model,df_test,df_sample)"
      ],
      "metadata": {
        "id": "kwAWuOmaABjb",
        "outputId": "5ad1c317-0b57-4748-8e1e-bacfe8f7afe4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "100% (2935 of 2935) |####################| Elapsed Time: 0:07:51 Time:  0:07:51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled = Fill_Cf_Matrix(model,user_s,item_s,df_sample['userID'].unique(),df_sample['itemID'].unique())"
      ],
      "metadata": {
        "id": "nuALg9eIArRd",
        "outputId": "2c14aa66-0803-46dc-aabb-471f80a6a2db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "883894/883894 [==============================] - 1196s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0% (63623 of 28284595) |               | Elapsed Time: 0:00:00 ETA:   0:01:28"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (28284595 of 28284595) |############| Elapsed Time: 0:01:31 Time:  0:01:31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled.to_csv(\"/content/gdrive/MyDrive/yelp SHOP/Yelp_shopping_GMF_filled_mat_0.7.csv\")"
      ],
      "metadata": {
        "id": "AeACXLoFaa1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/Yelp_shopping_GMF_filled_mat.csv\")"
      ],
      "metadata": {
        "id": "5STcxr2W2AKg",
        "outputId": "8d434b33-0b7c-4ac9-bdaa-eeae48733183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ef4718b7-ab27-496e-9e6d-0f92cc4dde46\", \"Yelp_shopping_GMF_filled_mat.csv\", 702280977)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}