{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CF_Social_Autoencoder.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmineSdk/RecommenderSystem/blob/main/Notebook/CF_Social_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download CellPhones -f CellPhonesRating.csv\n",
        "! kaggle datasets download yelp-shopping-rating-review -f review_shopping.csv"
      ],
      "metadata": {
        "id": "UM0vifCk9CB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a03ae5d7-c0e2-4426-e40c-4034419169a6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "CellPhonesRating.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Downloading review_shopping.csv.zip to /content\n",
            " 39% 9.00M/22.9M [00:01<00:02, 6.09MB/s]\n",
            "100% 22.9M/22.9M [00:01<00:00, 16.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TcGcg4ZzRQD1"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from scipy.sparse import csr_matrix\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import random\n",
        "from keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from progressbar import progressbar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = ['/content/CellPhonesRating_50K_reviews.csv','yelp_rating_mat','yelp_user_mat']\n",
        "MODEL_PATH = []\n",
        "PATH = ''\n",
        "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "precision = tf.keras.metrics.Precision()\n",
        "METRICS = ['accuracy','mae',rmse,precision]"
      ],
      "metadata": {
        "id": "MbQZ8yLYOnWI"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/review_shopping.csv.zip')"
      ],
      "metadata": {
        "id": "K_fZblYVAeGg"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = df\n",
        "#df_sample.to_csv('/content/CellPhonesRating_50K_reviews.csv')"
      ],
      "metadata": {
        "id": "xw_P6DJzAa7n"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample['productID'].nunique()"
      ],
      "metadata": {
        "id": "2YFUZLlNESK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample['reviewerID'].nunique()"
      ],
      "metadata": {
        "id": "i6pbXRh-EHDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/yelp-user-user-trust.zip\" -d \"/content/\"\n",
        "!unzip \"/content/yelp-review-ratings.zip\" -d \"/content/\""
      ],
      "metadata": {
        "id": "ppVGlIh8Ce8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DatasetToUserItemDataFrame(dataframe,userID,itemID,rating):\n",
        "  #Setting new item IDs from string to int \n",
        "  itemKeys = [] \n",
        "  i = 0\n",
        "  for item in dataframe[itemID].value_counts(sort=False):\n",
        "    temp = np.full((item),i)\n",
        "    itemKeys = np.append(itemKeys,temp)\n",
        "    i += 1\n",
        "\n",
        "  #Setting new user IDs from string to int\n",
        "  userKeysDic = {}\n",
        "  userKeys = np.zeros((dataframe[userID].size))\n",
        "  i = 0\n",
        "  for user in dataframe[userID].unique():\n",
        "    userKeysDic[user] = i\n",
        "    i += 1\n",
        "  i = 0\n",
        "  for user in dataframe[userID]:\n",
        "    userKeys[i] = userKeysDic[user]\n",
        "    i += 1\n",
        "\n",
        "  #Converting arrays from float to int \n",
        "  userKeys = userKeys.astype(int)\n",
        "  itemKeys = itemKeys.astype(int)\n",
        "\n",
        "  \n",
        "  user_item = csr_matrix((dataframe[rating].values.astype(int),(userKeys,itemKeys))) #Creating sparse matrix\n",
        "  user_item_matrix = user_item.toarray() #Converting sparse matrix into array\n",
        "  df_user_item = pd.DataFrame(user_item_matrix,index = dataframe[userID].unique()  ,columns = dataframe[itemID].unique() ) \n",
        "\n",
        "  return df_user_item"
      ],
      "metadata": {
        "id": "HHmzbMvDVB0b"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getUsersRatings(df,usersList): #returns df_rating containing only users that have friends \n",
        "  temp = df.copy()\n",
        "  for user in list(df['userID']):\n",
        "    if user not in usersList:\n",
        "      temp = temp.drop(temp.loc[temp['userID'] == user].index)\n",
        "  return temp"
      ],
      "metadata": {
        "id": "fbVQ62DroVDu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def genIndexColumn(df,min_index,max_index,min_column,max_column): #generates random int index and columns for a given df\n",
        "  New_User_IDs = random.sample(range(min_index,max_index),df.index.size)\n",
        "  New_Item_IDs = random.sample(range(min_column,max_column),df.columns.size)\n",
        "\n",
        "  df_new = pd.DataFrame(df.to_numpy(),index = New_User_IDs,columns = New_Item_IDs)\n",
        "  return df_new"
      ],
      "metadata": {
        "id": "5a5J7Mkt5yzm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generateIDs(df,index,columns,min_index,max_index,min_column,max_column):\n",
        "  users = df[index].unique()\n",
        "  items = df[columns].unique()\n",
        "  df_train = df.copy()\n",
        "\n",
        "  New_User_IDs = random.sample(range(min_index,max_index),df[index].nunique())\n",
        "  New_Item_IDs = random.sample(range(min_column,max_column),df[columns].nunique())\n",
        "  i = 0\n",
        "  for d in progressbar(users) :\n",
        "    df_train[index].replace({d : New_User_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "\n",
        "  i = 0\n",
        "  for d in progressbar(items) :\n",
        "    df_train[columns].replace({d : New_Item_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "  return df_train"
      ],
      "metadata": {
        "id": "mmzsdMDPmC8I"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processGmfData(df,index,column,min_user_id,max_user_id,min_item_id,max_item_id):\n",
        "  \n",
        "  \n",
        "  #df.rename(columns = {'text': 'reviewText', 'stars': 'rating', 'business_id': 'itemID', 'user_id': 'userID'}, inplace = True)\n",
        "  df.rename(columns = {'reviewerID':'userID', 'productID' : 'itemID'},inplace = True)\n",
        "  df = generateIDs(df,index,column,min_user_id,max_user_id,min_item_id,max_item_id)\n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "\n",
        "  df['one'] = df['rating'].apply(lambda x: 1 if x==1 else 0)\n",
        "  df['two'] = df['rating'].apply(lambda x: 1 if x==2 else 0)\n",
        "  df['three'] = df['rating'].apply(lambda x: 1 if x==3 else 0)\n",
        "  df['four'] = df['rating'].apply(lambda x: 1 if x==4 else 0)\n",
        "  df['five'] = df['rating'].apply(lambda x: 1 if x==5 else 0)\n",
        "  #df['six'] = df['rating'].apply(lambda x: 1 if x==6 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "08ifZdzgO2u9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GMF\n",
        "def getGMFmodel(num_users,num_items,SIZE_):\n",
        "\n",
        "  input_userID = layers.Input(shape=[1], name='user_ID')\n",
        "  input_itemID = layers.Input(shape=[1], name='item_ID')\n",
        "\n",
        "  user_emb_GMF = layers.Embedding(num_users, SIZE_, name='user_emb_GMF')(input_userID)\n",
        "  item_emb_GMF = layers.Embedding(num_items, SIZE_, name='item_emb_GMF')(input_itemID)\n",
        "\n",
        "  u_GMF = layers.Flatten()(user_emb_GMF)\n",
        "  i_GMF = layers.Flatten()(item_emb_GMF)\n",
        "\n",
        "  dot_layer = layers.Multiply()([u_GMF, i_GMF])\n",
        "\n",
        "  out_layer = layers.Dense(5, activation='softmax', name='output')(dot_layer)\n",
        "\n",
        "  GMF = tf.keras.Model([input_userID, input_itemID], out_layer)\n",
        "  \n",
        "  return GMF\n",
        "\n",
        "\n",
        "def user_item_ID_lists(userIDs,itemIDs):\n",
        "  i = 0\n",
        "  item_s = pd.Series()\n",
        "  user_s = pd.Series()\n",
        "  for item in progressbar(itemIDs):\n",
        "    temp = pd.Series(userIDs)\n",
        "    user_s = item_s.append(temp)\n",
        "    temp = []\n",
        "    temp = [item for user in userIDs]\n",
        "    temp = pd.Series(temp)\n",
        "    item_s = item_s.append(temp)\n",
        "\n",
        "  return user_s,item_s\n",
        "\n",
        "\n",
        "def Fill_Cf_Matrix(model,userList,itemList,userIDs,itemIDs):\n",
        "\n",
        "  prediction = model.predict([userList,itemList],verbose = 1)\n",
        "  i = 0\n",
        "  row = []\n",
        "  matrix = []\n",
        "  print(\"pred done\")\n",
        "  \n",
        "  for i in progressbar(range(userList.shape[0])):\n",
        "    result = np.where(prediction[i] == np.amax(prediction[i]))[0][0] + 1\n",
        "    row.append(result)\n",
        "    if len(row) == userIDs.size:\n",
        "      matrix.append(row)\n",
        "      row = []\n",
        "       \n",
        "    i += 1\n",
        "\n",
        "  matrix_arr = np.array(matrix)\n",
        "  matrix_arr = matrix_arr.transpose()\n",
        "  dataframe = pd.DataFrame(matrix_arr, index = userIDs, columns = itemIDs)\n",
        "  \n",
        "  return dataframe\n",
        "\n",
        "def loadDataset(fileID):\n",
        "  dataset = pd.read_csv(DATA[fileID])\n",
        "  print(\"== FILE LOADED ==\")\n",
        "  return dataset\n",
        "\n",
        "def saveDataframe(df,fileID):\n",
        "  file_path = PATH+'GMF_filled_'+DATA[fileID]\n",
        "  df.to_csv(file_path)\n",
        "\n",
        "def genGMFmat(df,df_test,modelID,fileID,nbrE,lossF,OF,emb,filter=None,cb=None):\n",
        "\n",
        "  # df_og = loadDataset(fileID)\n",
        "  # df = processGmfData(df_og,'userID','itemID',1000,2000+df_og['reviewerID'].nunique(),60000,61000+df_og['productID'].nunique())\n",
        "  \n",
        "  x_train ,x_test,y_train,y_test = train_test_split(df_test[['userID','itemID']],df_test[['one','two','three','four','five']],test_size=0.2,stratify=df_test[['one','two','three','four','five']])\n",
        "  model_trained = trainModel(modelID,nbrE,lossF,OF,[x_train['userID'],x_train['itemID']],y_train,maxUserID=df_test['userID'].max() + 1,maxItemID =df_test['itemID'].max() + 1,embed_size=emb)\n",
        "  model_trained.evaluate([x_test['userID'],x_test['itemID']],y_test)\n",
        "\n",
        "  #df_mat_filled = FillSparseMat(model_trained,df_test,df)\n",
        "  \n",
        "  return model_trained\n",
        "\n",
        "def FillSparseMat(model,df_prepro,df_og):\n",
        "\n",
        "  user_s,item_s = user_item_ID_lists(df_prepro['userID'].unique(),df_prepro['itemID'].unique())\n",
        "  #df_mat_filled = Fill_Cf_Matrix(model,user_s,item_s,df_og['userID'].unique(),df_og['itemID'].unique())\n",
        "\n",
        "  return user_s,item_s\n",
        "\n",
        "\n",
        "def trainModel(modelID,nbrEpochs,lossF,OF,x_train ,y_train ,mid_layer_ratio=None,nb_layers=None,maxUserID = None,maxItemID = None,embed_size = None,filter_size = None):\n",
        "  \n",
        "  if modelID =='BLCNN':\n",
        "    model = getBLCNNmodel(embed_size,filter_size)\n",
        "  if modelID == 'GMF':\n",
        "    model = getGMFmodel(maxUserID,maxItemID,embed_size)\n",
        "  elif modelID == 'S-AutoCF':\n",
        "    model = getAutoCFmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "  elif modelID == 'SS-AutoCF':\n",
        "    model = getSS_HAEmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "\n",
        "  model.compile(optimizer = OF,\n",
        "                    loss = lossF,\n",
        "                    metrics= METRICS)\n",
        "  model.fit(x_train,y_train,epochs = nbrEpochs,validation_split=0.14)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def getAutoCFmodel(x_train,mid_layer_ratio,nb_layers):\n",
        "  #mid_layer_ratio [0 - 1]\n",
        "  layer_ratio =  mid_layer_ratio + nb_layers*0.1\n",
        "\n",
        "  encoder_input = layers.Input(shape=(x_train.shape[1]),name='user_item')\n",
        "  flat = layers.Flatten()(encoder_input)\n",
        "  for i in range(nb_layers):\n",
        "    hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(hid_encoder)\n",
        "    layer_ratio -= 0.1\n",
        "  hid_encoder(flat)\n",
        "  encoder_output = layers.Dense(mid_layer_ratio*x_train.shape[1],activation=\"relu\")(hid_encoder)\n",
        "  for i in range(nb_layers):\n",
        "    decoder_input = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(decoder_input)\n",
        "    layer_ratio += 0.1\n",
        "  decoder_input(encoder_output)\n",
        "  decoder_output = layers.Dense(x_train.shape[1],activation=\"relu\")(decoder_input)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs = encoder_input, outputs = decoder_output)\n",
        "\n",
        "  return autoencoder\n",
        "\n",
        "def getSS_HAEmodel(x_train,mid_layer_ratio,nb_layers):\n",
        "  \n",
        "  #mid_layer_ratio [0 - 1]\n",
        "  layer_ratio =  mid_layer_ratio + nb_layers*0.1\n",
        "\n",
        "  #Social_Autoencoder\n",
        "\n",
        "  rating_input = layers.Input(shape=(x_train.shape[1]),name='user_item')\n",
        "  social_input = layers.Input(shape=(x_train.shape[1]),name='user_user')\n",
        "\n",
        "  flat_rating = layers.Flatten()(rating_input)\n",
        "  flat_social = layers.Flatten()(social_input)\n",
        "\n",
        "  #dropout = layers.Dropout(.2)(flat)\n",
        "  SharedLayer_encoder = layers.Concatenate()([flat_rating,flat_social])\n",
        "  for i in range(nb_layers):\n",
        "    hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(hid_encoder)\n",
        "    layer_ratio -= 0.1\n",
        "  hid_encoder(SharedLayer_encoder)\n",
        "  encoder_output = layers.Dense(mid_layer_ratio,activation=\"relu\")(hid_encoder)\n",
        "  for i in range(nb_layers):\n",
        "    hid_decoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(hid_decoder)\n",
        "    layer_ratio += 0.1\n",
        "  hid_decoder(encoder_output)\n",
        "  SharedLayer_decoder =  layers.Dense(df_mat_rating.shape[1]+df_mat_trust.shape[1],activation=\"relu\")(hid_decoder)\n",
        "  rating_decoded , social_decoded = tf.split(SharedLayer_decoder,[df_mat_rating.shape[1],df_mat_trust.shape[1]],1)\n",
        "\n",
        "  rating_output = layers.Dense(df_mat_rating.shape[1],activation=\"relu\",name='rating_output')(rating_decoded)\n",
        "  social_output = layers.Dense(df_mat_trust.shape[1],activation=\"relu\",name='social_output')(social_decoded)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs = [rating_input,social_input], outputs = [rating_output,social_output])\n",
        "\n",
        "  return autoencoder\n",
        "\n",
        "def createAutoCF(modelID,input_fileID,target_fileID,nbrEpochs,lossF,OF,mid_layer_ratio,nb_layers):\n",
        "\n",
        "  sparseDf = loadDataset(input_fileID)\n",
        "  df_mat_filled = loadDataset(target_fileID)\n",
        "  df_mat = DatasetToUserItemDataFrame(sparseDf,'userID','itemID','rating')\n",
        "  x_train,x_test,y_train,y_test = train_test_split(df_mat,df_mat_filled)\n",
        "  model_trained = trainModel(modelID,nbrEpochs,lossF,OF,x_train,y_train,mid_layer_ratio,nb_layers)\n",
        "\n",
        "  return model_trained , x_test, y_test\n",
        "\n",
        "def createSS_HAE(modelID,input_rating_fileID,target_rating_fileID,trust_fileID,nbrEpochs,lossF,OF,mid_layer_ratio,nb_layers):\n",
        "\n",
        "  sparseDf = loadDataset(input_rating_fileID)\n",
        "  df_rating_filled = loadDataset(target_rating_fileID)\n",
        "  df_trust_mat = loadDataset(trust_fileID)\n",
        "  trust_users_list = list(df_trust_mat.index)\n",
        "  df_rating_filled = getUsersRatings(df_rating_filled,trust_users_list) \n",
        "  df_rating_filled = orgDataframe(df_rating_filled,trust_users_list)\n",
        "\n",
        "  #input & target for autoencoder training\n",
        "  df_rating_mat = DatasetToUserItemDataFrame(sparseDf,'userID','itemID','rating')\n",
        "  df_rating_mat_filled = orgMatDataframe(df_rating_mat_filled,trust_users_list)\n",
        "\n",
        "  #data split\n",
        "  x_rating_train,x_rating_test,y_rating_train,y_rating_test = train_test_split(df_rating_mat,df_rating_mat_filled)\n",
        "  x_train,x_test = train_test_split(df_trust_mat)\n",
        "\n",
        "  model_trained = trainModel(modelID,nbrEpochs,lossF,OF,[x_rating_train,x_train],[y_rating_train,x_train],mid_layer_ratio,nb_layers)\n",
        "\n",
        "  return model_trained , [x_rating_test,y_rating_test] , x_test\n",
        "\n",
        "def evaluateModel(model,x_test,y_test):\n",
        "  model.evaluate(x_test,y_test) \n",
        "\n",
        "def orgDataframe(df_to_org,org_list):\n",
        "\n",
        "  df_org = pd.DataFrame(columns = ['userID','itemID','rating','reviewText'])\n",
        "  for user in org_list:\n",
        "    df_temp = df_to_org[(df_to_org['userID'] == user )]\n",
        "    df_org = pd.concat([df_org,df_temp])\n",
        "\n",
        "  return df_org\n",
        "\n",
        "def orgMatDataframe(df_to_org,org_list):\n",
        "  \n",
        "  df_org = pd.DataFrame(index = list(df_to_org.index),columns = list(df_to_org.columns))\n",
        "  for user in org_list:\n",
        "    df_org.loc[user] = list(df_to_org.loc[user])\n",
        "  \n",
        "  return df_org\n",
        "\n",
        "def getItemsScore(userID,modelID,fileID):\n",
        "  \n",
        "  listeItemScore = pd.DataFrame()\n",
        "  \n",
        "  if modelID == 0:\n",
        "    listItemScore = predSeCF(userID,modelID,fileID)   #CF\n",
        "  elif modelID == 1:\n",
        "    listItemScore = predSeCF(userID,modelID,fileID)   #CF + Sentiment\n",
        "  elif modelID == 2:\n",
        "    listItemScore = predSSeCF(userID,modelID,fileID)  #CF + Social\n",
        "  elif modelID == 3:\n",
        "    listItemScore = predSSeCF(userID,modelID,fileID)  #CF + Sentiment + Social\n",
        "\n",
        "  return listeItemScore \n",
        "\n",
        "def getModel(modelID):\n",
        "  model = tf.keras.models.load_model(MODEL_PATH[modelID])\n",
        "  return model\n",
        "\n",
        "def loadDataFrame(df):\n",
        "  df_loaded = pd.read_csv(DATA[df])\n",
        "  return  df_loaded\n",
        "\n",
        "def predSeCF(userID,modelID,fileID):\n",
        "  \n",
        "  model = getModel(modelID)\n",
        "  df_user_item_mat = pd.read_csv(DATA[fileID])\n",
        "  SparseScoresVec = df_user_item_mat.loc[userID].to_numpy()\n",
        "  listItemScores = df_user_item_mat.loc[userID].to_frame()\n",
        "  listItemScores = listItemScores.reset_index()\n",
        "  listItemScores.set_axis(['itemID','score'],axis='columns',inplace=True)\n",
        "  PredScoresVec = model.predict(SparseScoresVec)\n",
        "  listItemScores.replace(SparseScoresVec,PredScoresVec,inplace = True)\n",
        "\n",
        "  return listItemScores\n",
        "\n",
        "def predSSeCF(userID,modelID,ratings_fileID,trust_fileID):\n",
        "\n",
        "  model = getModel(modelID)\n",
        "  df_user_item_mat = loadDataset(fileID)\n",
        "  df_user_user_mat = pd.read_csv(trust_fileID)\n",
        "  SparseScoresVec = df_user_item_mat.loc[userID].to_numpy()\n",
        "  SparseTrustVec = df_user_user_mat.loc[userID].to_numpy()\n",
        "  listItemScores = df_user_item_mat.loc[userID].to_frame()\n",
        "  listItemScores = listItemScores.reset_index()\n",
        "  listItemScores.set_axis(['itemID','score'],axis='columns',inplace=True)\n",
        "  PredScoresVec,SparseTrustVec = model.predict([SparseScoresVec,SparseTrustVec])\n",
        "  listItemScores.replace(SparseScoresVec,PredScoresVec,inplace = True)\n",
        "\n",
        "  return listItemScores\n",
        "  "
      ],
      "metadata": {
        "id": "z6KHUFF9gwT5"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datapreprocess(df_sample):\n",
        "  \n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df_sample = df_sample.dropna(how='any',axis=0)\n",
        "  df_sample.drop(columns=['funny', 'cool','review_id','useful'],inplace=True)\n",
        "  df_sample.rename(columns = {'user_id':'userID', 'business_id' : 'itemID', 'text':'reviewText','stars':'rating'},inplace = True)\n",
        "  df_sample.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  df_sample = generateIDs(df_sample,'userID','itemID',1000,2000+df_sample['userID'].nunique(),60000,61000+df_sample['itemID'].nunique())\n",
        "  \n",
        "  \n",
        "\n",
        "  df_sample['one'] = df_sample['rating'].apply(lambda x: 1 if x==1 else 0)\n",
        "  df_sample['two'] = df_sample['rating'].apply(lambda x: 1 if x==2 else 0)\n",
        "  df_sample['three'] = df_sample['rating'].apply(lambda x: 1 if x==3 else 0)\n",
        "  df_sample['four'] = df_sample['rating'].apply(lambda x: 1 if x==4 else 0)\n",
        "  df_sample['five'] = df_sample['rating'].apply(lambda x: 1 if x==5 else 0)\n",
        "  #df['six'] = df['rating'].apply(lambda x: 1 if x==6 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df_sample"
      ],
      "metadata": {
        "id": "EHC86cO8GJcK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.head()"
      ],
      "metadata": {
        "id": "zA_oE3XPXfBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = df_sample.dropna(how='any',axis=0)"
      ],
      "metadata": {
        "id": "WPaVB0D7XaT2"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = datapreprocess(df_sample)"
      ],
      "metadata": {
        "id": "c3PUFoU5GrhR",
        "outputId": "063e1b1c-7b43-4819-af7b-983e0771ab11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0% (23 of 2935) |                      | Elapsed Time: 0:00:00 ETA:   0:00:13"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== PREPROCESSING DATA ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (2935 of 2935) |####################| Elapsed Time: 0:00:16 Time:  0:00:16\n",
            "100% (9637 of 9637) |####################| Elapsed Time: 0:01:35 Time:  0:01:35\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== DATA PREPROCESSED ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "92GlfJmlX_aB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del df"
      ],
      "metadata": {
        "id": "4dsJvtBsrSs2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genGMFmat(df_sample,df_test,'GMF',0,5,tf.keras.losses.CategoricalCrossentropy(),'adam',32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ9Cm1zW_lDY",
        "outputId": "a5f69368-9962-4f6c-b84b-e98a1c79b829"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1232/1232 [==============================] - 9s 6ms/step - loss: 1.4506 - accuracy: 0.3641 - mae: 0.3015 - root_mean_squared_error: 0.3837 - precision: 0.4546 - val_loss: 1.3879 - val_accuracy: 0.3689 - val_mae: 0.2905 - val_root_mean_squared_error: 0.3781 - val_precision: 0.0000e+00\n",
            "Epoch 2/5\n",
            "1232/1232 [==============================] - 7s 6ms/step - loss: 1.3466 - accuracy: 0.4705 - mae: 0.2832 - root_mean_squared_error: 0.3724 - precision: 0.8421 - val_loss: 1.3767 - val_accuracy: 0.3738 - val_mae: 0.2852 - val_root_mean_squared_error: 0.3774 - val_precision: 0.5000\n",
            "Epoch 3/5\n",
            "1232/1232 [==============================] - 7s 6ms/step - loss: 1.0225 - accuracy: 0.7066 - mae: 0.2327 - root_mean_squared_error: 0.3176 - precision: 0.9639 - val_loss: 1.4246 - val_accuracy: 0.3632 - val_mae: 0.2832 - val_root_mean_squared_error: 0.3839 - val_precision: 0.4102\n",
            "Epoch 4/5\n",
            "1232/1232 [==============================] - 8s 7ms/step - loss: 0.5538 - accuracy: 0.8519 - mae: 0.1436 - root_mean_squared_error: 0.2244 - precision: 0.9739 - val_loss: 1.5932 - val_accuracy: 0.3452 - val_mae: 0.2841 - val_root_mean_squared_error: 0.3991 - val_precision: 0.3871\n",
            "Epoch 5/5\n",
            "1232/1232 [==============================] - 7s 6ms/step - loss: 0.2672 - accuracy: 0.9554 - mae: 0.0783 - root_mean_squared_error: 0.1481 - precision: 0.9876 - val_loss: 1.8158 - val_accuracy: 0.3349 - val_mae: 0.2844 - val_root_mean_squared_error: 0.4137 - val_precision: 0.3701\n",
            "358/358 [==============================] - 1s 4ms/step - loss: 1.7898 - accuracy: 0.3486 - mae: 0.2817 - root_mean_squared_error: 0.4109 - precision: 0.3828\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_s,item_s = FillSparseMat(model,df_test,df_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwAWuOmaABjb",
        "outputId": "59b7327d-b39e-40ab-ce46-fbc1e3585d4b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "100% (2233 of 2233) |####################| Elapsed Time: 0:12:50 Time:  0:12:50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled = Fill_Cf_Matrix(model,user_s,item_s,df_sample['userID'].unique(),df_sample['itemID'].unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nuALg9eIArRd",
        "outputId": "b0d61f37-0e83-4d00-f03e-6e46010aa04f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1798961/1798961 [==============================] - 2572s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0% (16495 of 57566740) |               | Elapsed Time: 0:00:00 ETA:   0:11:38"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (57566740 of 57566740) |############| Elapsed Time: 0:11:32 Time:  0:11:32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled.to_csv(\"/content/Aamazon_Cellphones_35k_GMF_filled_mat.csv\")"
      ],
      "metadata": {
        "id": "3b7mpg60dEsv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/Aamazon_Cellphones_35k_GMF_filled_mat.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "5STcxr2W2AKg",
        "outputId": "2f6dd2ca-0d86-42cd-9d0c-2279f4607978"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_92fce5a4-2f5b-4492-ac24-5e8af71a3bd9\", \"Aamazon_Cellphones_35k_GMF_filled_mat.csv\", 115538134)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}