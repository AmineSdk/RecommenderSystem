{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CF_Social_Autoencoder.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmineSdk/RecommenderSystem/blob/main/Notebook/CF_Social_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download yelp-restaurants-review-rating -f review_restaurant.csv\n",
        "# ! kaggle datasets download Yelp-shopping-GMF-filled-mat -f Yelp_shopping_GMF_filled_mat.csv\n",
        "# ! kaggle datasets download yelp-shopping-rating-review -f review_shopping.csv\n",
        "#! kaggle datasets download Aamazon-Cellphones-35k-GMF-filled-mat -f Aamazon_Cellphones_35k_GMF_filled_mat.csv\n",
        "! kaggle datasets download Yelp-restaurant-GMF-filled-mat -f Yelp_restaurant_GMF_filled_mat.csv"
      ],
      "metadata": {
        "id": "UM0vifCk9CB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f0ec0b-6c68-4090-8a37-2079d9fbc9a1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "review_restaurant.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Yelp_restaurant_GMF_filled_mat.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TcGcg4ZzRQD1"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from scipy.sparse import csr_matrix\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import random\n",
        "from keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from progressbar import progressbar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def amazonPreprocess(df):\n",
        "  \n",
        "  df = df[:35000]\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.rename(columns = {'reviewerID':'userID', 'productID' : 'itemID'},inplace = True)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  print(\"DATASET PREPROCESSED\")\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "lAZGdMRODRqE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yelpPreprocess(df):\n",
        "\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.drop(columns=['funny', 'cool','review_id','useful','date'],inplace=True)\n",
        "  df.rename(columns = {'user_id':'userID', 'business_id':'itemID','stars':'rating'},inplace = True)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  print(\"DATASET PREPROCESSED\")\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "03zm3UHNDu8I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DatasetToUserItemDataFrame(dataframe,userID,itemID,rating):\n",
        "  #Setting new item IDs from string to int \n",
        "  itemKeys = [] \n",
        "  i = 0\n",
        "  for item in dataframe[itemID].value_counts(sort=False):\n",
        "    temp = np.full((item),i)\n",
        "    itemKeys = np.append(itemKeys,temp)\n",
        "    i += 1\n",
        "\n",
        "  #Setting new user IDs from string to int\n",
        "  userKeysDic = {}\n",
        "  userKeys = np.zeros((dataframe[userID].size))\n",
        "  i = 0\n",
        "  for user in dataframe[userID].unique():\n",
        "    userKeysDic[user] = i\n",
        "    i += 1\n",
        "  i = 0\n",
        "  for user in dataframe[userID]:\n",
        "    userKeys[i] = userKeysDic[user]\n",
        "    i += 1\n",
        "\n",
        "  #Converting arrays from float to int \n",
        "  userKeys = userKeys.astype(int)\n",
        "  itemKeys = itemKeys.astype(int)\n",
        "\n",
        "  \n",
        "  user_item = csr_matrix((dataframe[rating].values.astype(int),(userKeys,itemKeys))) #Creating sparse matrix\n",
        "  user_item_matrix = user_item.toarray() #Converting sparse matrix into array\n",
        "  df_user_item = pd.DataFrame(user_item_matrix,index = dataframe[userID].unique()  ,columns = dataframe[itemID].unique() ) \n",
        "\n",
        "  return df_user_item\n",
        "\n",
        "def getUsersRatings(df,usersList): #returns df_rating containing only users that have friends \n",
        "  temp = df.copy()\n",
        "  for user in list(df['userID']):\n",
        "    if user not in usersList:\n",
        "      temp = temp.drop(temp.loc[temp['userID'] == user].index)\n",
        "  return temp\n",
        "\n",
        "def genIndexColumn(df,min_index,max_index,min_column,max_column): #generates random int index and columns for a given df\n",
        "  New_User_IDs = random.sample(range(min_index,max_index),df.index.size)\n",
        "  New_Item_IDs = random.sample(range(min_column,max_column),df.columns.size)\n",
        "\n",
        "  df_new = pd.DataFrame(df.to_numpy(),index = New_User_IDs,columns = New_Item_IDs)\n",
        "  return df_new\n",
        "\n",
        "def generateIDs(df,index,columns,min_index,max_index,min_column,max_column):\n",
        "  users = df[index].unique()\n",
        "  items = df[columns].unique()\n",
        "  df_train = df.copy()\n",
        "\n",
        "  New_User_IDs = random.sample(range(min_index,max_index),df[index].nunique())\n",
        "  New_Item_IDs = random.sample(range(min_column,max_column),df[columns].nunique())\n",
        "  i = 0\n",
        "  for d in progressbar(users) :\n",
        "    df_train[index].replace({d : New_User_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "\n",
        "  i = 0\n",
        "  for d in progressbar(items) :\n",
        "    df_train[columns].replace({d : New_Item_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "  return df_train\n",
        "\n",
        "def processGmfData(df,index,column,min_user_id,max_user_id,min_item_id,max_item_id):\n",
        "  \n",
        "  \n",
        "  #df.rename(columns = {'text': 'reviewText', 'stars': 'rating', 'business_id': 'itemID', 'user_id': 'userID'}, inplace = True)\n",
        "  df.rename(columns = {'reviewerID':'userID', 'productID' : 'itemID'},inplace = True)\n",
        "  df = generateIDs(df,index,column,min_user_id,max_user_id,min_item_id,max_item_id)\n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "\n",
        "  df['one'] = df['rating'].apply(lambda x: 1 if x==1 else 0)\n",
        "  df['two'] = df['rating'].apply(lambda x: 1 if x==2 else 0)\n",
        "  df['three'] = df['rating'].apply(lambda x: 1 if x==3 else 0)\n",
        "  df['four'] = df['rating'].apply(lambda x: 1 if x==4 else 0)\n",
        "  df['five'] = df['rating'].apply(lambda x: 1 if x==5 else 0)\n",
        "  #df['six'] = df['rating'].apply(lambda x: 1 if x==6 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "08ifZdzgO2u9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GMF\n",
        "def getGMFmodel(num_users,num_items,SIZE_):\n",
        "\n",
        "  input_userID = layers.Input(shape=[1], name='user_ID')\n",
        "  input_itemID = layers.Input(shape=[1], name='item_ID')\n",
        "\n",
        "  user_emb_GMF = layers.Embedding(num_users, SIZE_, name='user_emb_GMF')(input_userID)\n",
        "  item_emb_GMF = layers.Embedding(num_items, SIZE_, name='item_emb_GMF')(input_itemID)\n",
        "\n",
        "  u_GMF = layers.Flatten()(user_emb_GMF)\n",
        "  i_GMF = layers.Flatten()(item_emb_GMF)\n",
        "\n",
        "  dot_layer = layers.Multiply()([u_GMF, i_GMF])\n",
        "\n",
        "  out_layer = layers.Dense(5, activation='softmax', name='output')(dot_layer)\n",
        "\n",
        "  GMF = tf.keras.Model([input_userID, input_itemID], out_layer)\n",
        "  \n",
        "  return GMF\n",
        "\n",
        "\n",
        "def user_item_ID_lists(userIDs,itemIDs):\n",
        "  i = 0\n",
        "  item_s = pd.Series()\n",
        "  user_s = pd.Series()\n",
        "\n",
        "  if userIDs.size > itemIDs.size:\n",
        "    for item in progressbar(itemIDs):\n",
        "      temp = pd.Series(userIDs)\n",
        "      user_s = user_s.append(temp)\n",
        "      temp = []\n",
        "      temp = [item for user in userIDs]\n",
        "      temp = pd.Series(temp)\n",
        "      item_s = item_s.append(temp)\n",
        "  else:\n",
        "    for user in progressbar(userIDs):\n",
        "      temp = pd.Series(itemIDs)\n",
        "      item_s = item_s.append(temp)\n",
        "      temp = []\n",
        "      temp = [user for item in itemIDs]\n",
        "      temp = pd.Series(temp)\n",
        "      user_s = user_s.append(temp)\n",
        "\n",
        "  return user_s,item_s\n",
        "\n",
        "\n",
        "def Fill_Cf_Matrix(model,userList,itemList,userIDs,itemIDs):\n",
        "  rowLen = userIDs.size if userIDs.size > itemIDs.size else itemIDs.size\n",
        "\n",
        "  prediction = model.predict([userList,itemList],verbose = 1)\n",
        "  i = 0\n",
        "  row = []\n",
        "  matrix = []\n",
        "  print(\"pred done\")\n",
        "  \n",
        "  for i in progressbar(range(userList.shape[0])):\n",
        "    result = np.where(prediction[i] == np.amax(prediction[i]))[0][0] + 1\n",
        "    row.append(result)\n",
        "    if len(row) == rowLen:\n",
        "      matrix.append(row)\n",
        "      row = []\n",
        "       \n",
        "    i += 1\n",
        "\n",
        "  matrix_arr = np.array(matrix)\n",
        "  if(userIDs.size > itemIDs.size):\n",
        "    matrix_arr = matrix_arr.transpose()\n",
        "  dataframe = pd.DataFrame(matrix_arr, index = userIDs, columns = itemIDs)\n",
        "  \n",
        "  return dataframe\n",
        "\n",
        "def loadDataset(fileID):\n",
        "  dataset = pd.read_csv(DATA[fileID])\n",
        "  print(\"== FILE LOADED ==\")\n",
        "  return dataset\n",
        "\n",
        "def saveDataframe(df,fileID):\n",
        "  file_path = PATH+'GMF_filled_'+DATA[fileID]\n",
        "  df.to_csv(file_path)\n",
        "\n",
        "def genGMFmat(df,df_test,modelID,fileID,nbrE,lossF,OF,emb,filter=None,cb=None):\n",
        "\n",
        "  # df_og = loadDataset(fileID)\n",
        "  # df = processGmfData(df_og,'userID','itemID',1000,2000+df_og['reviewerID'].nunique(),60000,61000+df_og['productID'].nunique())\n",
        "  \n",
        "  x_train ,x_test,y_train,y_test = train_test_split(df_test[['userID','itemID']],df_test[['one','two','three','four','five']],test_size=0.2,stratify=df_test[['one','two','three','four','five']])\n",
        "  model_trained = trainModel(modelID,nbrE,lossF,OF,[x_train['userID'],x_train['itemID']],y_train,maxUserID=df_test['userID'].max() + 1,maxItemID =df_test['itemID'].max() + 1,embed_size=emb)\n",
        "  model_trained.evaluate([x_test['userID'],x_test['itemID']],y_test)\n",
        "\n",
        "  #df_mat_filled = FillSparseMat(model_trained,df_test,df)\n",
        "  \n",
        "  return model_trained\n",
        "\n",
        "def FillSparseMat(model,df_prepro,df_og):\n",
        "\n",
        "  user_s,item_s = user_item_ID_lists(df_prepro['userID'].unique(),df_prepro['itemID'].unique())\n",
        "  #df_mat_filled = Fill_Cf_Matrix(model,user_s,item_s,df_og['userID'].unique(),df_og['itemID'].unique())\n",
        "\n",
        "  return user_s,item_s\n",
        "\n",
        "\n",
        "def trainModel(modelID,nbrEpochs,lossF,OF,x_train ,y_train ,mid_layer_ratio=None,nb_layers=None,maxUserID = None,maxItemID = None,embed_size = None,filter_size = None,bs = 32):\n",
        "  \n",
        "  if modelID =='BLCNN':\n",
        "    model = getBLCNNmodel(embed_size,filter_size)\n",
        "  if modelID == 'GMF':\n",
        "    model = getGMFmodel(maxUserID,maxItemID,embed_size)\n",
        "  elif modelID == 'S-AutoCF':\n",
        "    model = getAutoCFmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "    print(model.summary())\n",
        "  elif modelID == 'SS-AutoCF':\n",
        "    model = getSS_HAEmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "\n",
        "  model.compile(optimizer = OF,\n",
        "                    loss = lossF,\n",
        "                    metrics= METRICS)\n",
        "  model.fit(x_train,y_train,epochs = nbrEpochs,batch_size = bs,validation_split=0.14)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def getAutoCFmodel(x_train,mid_layer_ratio,nb_layers):\n",
        "  #mid_layer_ratio [0 - 1]\n",
        "  layer_ratio =  mid_layer_ratio + nb_layers*0.05\n",
        "\n",
        "  encoder_input = layers.Input(shape=(x_train.shape[1]),name='user_item')\n",
        "  flat = layers.Flatten()(encoder_input)\n",
        "\n",
        "  if nb_layers != 1:\n",
        "    hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(flat)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio -= 0.05\n",
        "      hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(hid_encoder)\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*x_train.shape[1],activation=\"relu\")(hid_encoder)\n",
        "  else:\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*x_train.shape[1],activation=\"relu\")(flat)\n",
        "\n",
        "  if nb_layers != 1:\n",
        "    decoder_input = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(encoder_output)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio += 0.05\n",
        "      decoder_input = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(decoder_input)\n",
        "    decoder_output = layers.Dense(x_train.shape[1],activation=\"relu\")(decoder_input)\n",
        "  else:\n",
        "    decoder_output = layers.Dense(x_train.shape[1],activation=\"relu\")(encoder_output)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs = encoder_input, outputs = decoder_output)\n",
        "\n",
        "  return autoencoder\n",
        "\n",
        "def getSS_HAEmodel(x_train,mid_layer_ratio,nb_layers):\n",
        "  \n",
        "  #mid_layer_ratio [0 - 1]\n",
        "  layer_ratio =  mid_layer_ratio + nb_layers*0.1\n",
        "\n",
        "  #Social_Autoencoder\n",
        "\n",
        "  rating_input = layers.Input(shape=(x_train.shape[1]),name='user_item')\n",
        "  social_input = layers.Input(shape=(x_train.shape[1]),name='user_user')\n",
        "\n",
        "  flat_rating = layers.Flatten()(rating_input)\n",
        "  flat_social = layers.Flatten()(social_input)\n",
        "\n",
        "  #dropout = layers.Dropout(.2)(flat)\n",
        "  SharedLayer_encoder = layers.Concatenate()([flat_rating,flat_social])\n",
        "  for i in range(nb_layers):\n",
        "    hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(hid_encoder)\n",
        "    layer_ratio -= 0.1\n",
        "  hid_encoder(SharedLayer_encoder)\n",
        "  encoder_output = layers.Dense(mid_layer_ratio,activation=\"relu\")(hid_encoder)\n",
        "  for i in range(nb_layers):\n",
        "    hid_decoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(hid_decoder)\n",
        "    layer_ratio += 0.1\n",
        "  hid_decoder(encoder_output)\n",
        "  SharedLayer_decoder =  layers.Dense(df_mat_rating.shape[1]+df_mat_trust.shape[1],activation=\"relu\")(hid_decoder)\n",
        "  rating_decoded , social_decoded = tf.split(SharedLayer_decoder,[df_mat_rating.shape[1],df_mat_trust.shape[1]],1)\n",
        "\n",
        "  rating_output = layers.Dense(df_mat_rating.shape[1],activation=\"relu\",name='rating_output')(rating_decoded)\n",
        "  social_output = layers.Dense(df_mat_trust.shape[1],activation=\"relu\",name='social_output')(social_decoded)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs = [rating_input,social_input], outputs = [rating_output,social_output])\n",
        "\n",
        "  return autoencoder\n",
        "\n",
        "def createAutoCF(modelID,input_fileID,target_fileID,nbrEpochs,lossF,OF,mid_layer_ratio,nb_layers,bs):\n",
        "\n",
        "  sparseDf = loadDataset(input_fileID)\n",
        "  sparseDf = yelpPreprocess(sparseDf)\n",
        "  df_mat_filled = loadDataset(target_fileID)\n",
        "  df_mat_filled = df_mat_filled.set_index('Unnamed: 0')\n",
        "  df_mat = DatasetToUserItemDataFrame(sparseDf,'userID','itemID','rating')\n",
        "  if df_mat.shape[0]< df_mat.shape[1]:\n",
        "    df_mat = df_mat.T\n",
        "    df_mat_filled = df_mat_filled.T\n",
        "    print(df_mat_filled.shape,df_mat.shape)\n",
        "  x_train,x_test,y_train,y_test = train_test_split(df_mat,df_mat_filled,test_size = 0.2)\n",
        "  model_trained = trainModel(modelID,nbrEpochs,lossF,OF,x_train,y_train,mid_layer_ratio,nb_layers,bs = bs)\n",
        "\n",
        "  return model_trained , x_test, y_test\n",
        "\n",
        "def createSS_HAE(modelID,input_rating_fileID,target_rating_fileID,trust_fileID,nbrEpochs,lossF,OF,mid_layer_ratio,nb_layers):\n",
        "\n",
        "  sparseDf = loadDataset(input_rating_fileID)\n",
        "  df_rating_filled = loadDataset(target_rating_fileID)\n",
        "  df_trust_mat = loadDataset(trust_fileID)\n",
        "  trust_users_list = list(df_trust_mat.index)\n",
        "  df_rating_filled = getUsersRatings(df_rating_filled,trust_users_list) \n",
        "  df_rating_filled = orgDataframe(df_rating_filled,trust_users_list)\n",
        "\n",
        "  #input & target for autoencoder training\n",
        "  df_rating_mat = DatasetToUserItemDataFrame(sparseDf,'userID','itemID','rating')\n",
        "  df_rating_mat_filled = orgMatDataframe(df_rating_mat_filled,trust_users_list)\n",
        "\n",
        "  #data split\n",
        "  x_rating_train,x_rating_test,y_rating_train,y_rating_test = train_test_split(df_rating_mat,df_rating_mat_filled)\n",
        "  x_train,x_test = train_test_split(df_trust_mat)\n",
        "\n",
        "  model_trained = trainModel(modelID,nbrEpochs,lossF,OF,[x_rating_train,x_train],[y_rating_train,x_train],mid_layer_ratio,nb_layers)\n",
        "\n",
        "  return model_trained , [x_rating_test,y_rating_test] , x_test\n",
        "\n",
        "def evaluateModel(model,x_test,y_test):\n",
        "  model.evaluate(x_test,y_test) \n",
        "\n",
        "def orgDataframe(df_to_org,org_list):\n",
        "\n",
        "  df_org = pd.DataFrame(columns = ['userID','itemID','rating','reviewText'])\n",
        "  for user in org_list:\n",
        "    df_temp = df_to_org[(df_to_org['userID'] == user )]\n",
        "    df_org = pd.concat([df_org,df_temp])\n",
        "\n",
        "  return df_org\n",
        "\n",
        "def orgMatDataframe(df_to_org,org_list):\n",
        "  \n",
        "  df_org = pd.DataFrame(index = list(df_to_org.index),columns = list(df_to_org.columns))\n",
        "  for user in org_list:\n",
        "    df_org.loc[user] = list(df_to_org.loc[user])\n",
        "  \n",
        "  return df_org\n",
        "\n",
        "def getItemsScore(userID,modelID,fileID):\n",
        "  \n",
        "  listeItemScore = pd.DataFrame()\n",
        "  \n",
        "  if modelID == 0:\n",
        "    listItemScore = predSeCF(userID,modelID,fileID)   #CF\n",
        "  elif modelID == 1:\n",
        "    listItemScore = predSeCF(userID,modelID,fileID)   #CF + Sentiment\n",
        "  elif modelID == 2:\n",
        "    listItemScore = predSSeCF(userID,modelID,fileID)  #CF + Social\n",
        "  elif modelID == 3:\n",
        "    listItemScore = predSSeCF(userID,modelID,fileID)  #CF + Sentiment + Social\n",
        "\n",
        "  return listeItemScore \n",
        "\n",
        "def getModel(modelID):\n",
        "  model = tf.keras.models.load_model(MODEL_PATH[modelID])\n",
        "  return model\n",
        "\n",
        "def loadDataFrame(df):\n",
        "  df_loaded = pd.read_csv(DATA[df])\n",
        "  return  df_loaded\n",
        "\n",
        "def predSeCF(userID,modelID,fileID):\n",
        "  \n",
        "  model = getModel(modelID)\n",
        "  df_user_item_mat = pd.read_csv(DATA[fileID])\n",
        "  SparseScoresVec = df_user_item_mat.loc[userID].to_numpy()\n",
        "  listItemScores = df_user_item_mat.loc[userID].to_frame()\n",
        "  listItemScores = listItemScores.reset_index()\n",
        "  listItemScores.set_axis(['itemID','score'],axis='columns',inplace=True)\n",
        "  PredScoresVec = model.predict(SparseScoresVec)\n",
        "  listItemScores.replace(SparseScoresVec,PredScoresVec,inplace = True)\n",
        "\n",
        "  return listItemScores\n",
        "\n",
        "def predSSeCF(userID,modelID,ratings_fileID,trust_fileID):\n",
        "\n",
        "  model = getModel(modelID)\n",
        "  df_user_item_mat = loadDataset(fileID)\n",
        "  df_user_user_mat = pd.read_csv(trust_fileID)\n",
        "  SparseScoresVec = df_user_item_mat.loc[userID].to_numpy()\n",
        "  SparseTrustVec = df_user_user_mat.loc[userID].to_numpy()\n",
        "  listItemScores = df_user_item_mat.loc[userID].to_frame()\n",
        "  listItemScores = listItemScores.reset_index()\n",
        "  listItemScores.set_axis(['itemID','score'],axis='columns',inplace=True)\n",
        "  PredScoresVec,SparseTrustVec = model.predict([SparseScoresVec,SparseTrustVec])\n",
        "  listItemScores.replace(SparseScoresVec,PredScoresVec,inplace = True)\n",
        "\n",
        "  return listItemScores\n",
        "  "
      ],
      "metadata": {
        "id": "z6KHUFF9gwT5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datapreprocess(df_sample):\n",
        "  \n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df_sample = generateIDs(df_sample,'userID','itemID',1000,2000+df_sample['userID'].nunique(),60000,61000+df_sample['itemID'].nunique())\n",
        "  \n",
        "  df_sample['one'] = df_sample['rating'].apply(lambda x: 1 if x==1 else 0)\n",
        "  df_sample['two'] = df_sample['rating'].apply(lambda x: 1 if x==2 else 0)\n",
        "  df_sample['three'] = df_sample['rating'].apply(lambda x: 1 if x==3 else 0)\n",
        "  df_sample['four'] = df_sample['rating'].apply(lambda x: 1 if x==4 else 0)\n",
        "  df_sample['five'] = df_sample['rating'].apply(lambda x: 1 if x==5 else 0)\n",
        "  #df['six'] = df['rating'].apply(lambda x: 1 if x==6 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df_sample"
      ],
      "metadata": {
        "id": "EHC86cO8GJcK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = ['/content/review_shopping.csv.zip','/content/Yelp_shopping_GMF_filled_mat.csv.zip','/content/review_restaurant.csv.zip','/content/Yelp_restaurant_GMF_filled_mat.csv.zip']\n",
        "MODEL_PATH = []\n",
        "PATH = ''\n",
        "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "precision = tf.keras.metrics.Precision()\n",
        "METRICS = ['accuracy','mae',rmse]"
      ],
      "metadata": {
        "id": "3eS1ZsrzFU-m"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,x_test,y_test = createAutoCF('S-AutoCF',2,3,50,tf.keras.losses.MeanSquaredError(),'rmsprop',0.3,2,8)"
      ],
      "metadata": {
        "id": "jWu3NfsAGS2b",
        "outputId": "9b75d0d0-f9aa-4f23-e490-d8455e2234ca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== FILE LOADED ==\n",
            "DATASET PREPROCESSED\n",
            "== FILE LOADED ==\n",
            "(15588, 4346) (15588, 4346)\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " user_item (InputLayer)      [(None, 4346)]            0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4346)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1738)              7555086   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1521)              2645019   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1303)              1983166   \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1521)              1983384   \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1738)              2645236   \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4346)              7557694   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,369,585\n",
            "Trainable params: 24,369,585\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/50\n",
            "1341/1341 [==============================] - 23s 15ms/step - loss: 4.4035 - accuracy: 9.3249e-05 - mae: 1.4039 - root_mean_squared_error: 2.0985 - val_loss: 1.3389 - val_accuracy: 0.0000e+00 - val_mae: 0.8760 - val_root_mean_squared_error: 1.1571\n",
            "Epoch 2/50\n",
            "1341/1341 [==============================] - 21s 16ms/step - loss: 1.3381 - accuracy: 0.0000e+00 - mae: 0.8703 - root_mean_squared_error: 1.1568 - val_loss: 1.3343 - val_accuracy: 0.0000e+00 - val_mae: 0.8487 - val_root_mean_squared_error: 1.1551\n",
            "Epoch 3/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.3079 - accuracy: 0.0000e+00 - mae: 0.8608 - root_mean_squared_error: 1.1436 - val_loss: 1.2929 - val_accuracy: 0.0000e+00 - val_mae: 0.8458 - val_root_mean_squared_error: 1.1371\n",
            "Epoch 4/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.2847 - accuracy: 0.0000e+00 - mae: 0.8541 - root_mean_squared_error: 1.1335 - val_loss: 1.3194 - val_accuracy: 0.0000e+00 - val_mae: 0.8414 - val_root_mean_squared_error: 1.1486\n",
            "Epoch 5/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.2718 - accuracy: 0.0000e+00 - mae: 0.8502 - root_mean_squared_error: 1.1277 - val_loss: 1.2665 - val_accuracy: 0.0000e+00 - val_mae: 0.8540 - val_root_mean_squared_error: 1.1254\n",
            "Epoch 6/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.2570 - accuracy: 0.0000e+00 - mae: 0.8463 - root_mean_squared_error: 1.1211 - val_loss: 1.2723 - val_accuracy: 0.0000e+00 - val_mae: 0.8251 - val_root_mean_squared_error: 1.1280\n",
            "Epoch 7/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.2339 - accuracy: 0.0000e+00 - mae: 0.8411 - root_mean_squared_error: 1.1108 - val_loss: 1.2364 - val_accuracy: 0.0000e+00 - val_mae: 0.8635 - val_root_mean_squared_error: 1.1119\n",
            "Epoch 8/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.2075 - accuracy: 0.0000e+00 - mae: 0.8354 - root_mean_squared_error: 1.0989 - val_loss: 1.2280 - val_accuracy: 0.0000e+00 - val_mae: 0.8228 - val_root_mean_squared_error: 1.1082\n",
            "Epoch 9/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.1942 - accuracy: 0.0000e+00 - mae: 0.8325 - root_mean_squared_error: 1.0928 - val_loss: 1.2009 - val_accuracy: 0.0000e+00 - val_mae: 0.8331 - val_root_mean_squared_error: 1.0959\n",
            "Epoch 10/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.1710 - accuracy: 0.0000e+00 - mae: 0.8275 - root_mean_squared_error: 1.0821 - val_loss: 1.1971 - val_accuracy: 0.0000e+00 - val_mae: 0.8580 - val_root_mean_squared_error: 1.0941\n",
            "Epoch 11/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.1613 - accuracy: 0.0000e+00 - mae: 0.8248 - root_mean_squared_error: 1.0777 - val_loss: 1.1793 - val_accuracy: 0.0000e+00 - val_mae: 0.8212 - val_root_mean_squared_error: 1.0860\n",
            "Epoch 12/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.1407 - accuracy: 0.0000e+00 - mae: 0.8201 - root_mean_squared_error: 1.0680 - val_loss: 1.1588 - val_accuracy: 0.0000e+00 - val_mae: 0.8288 - val_root_mean_squared_error: 1.0765\n",
            "Epoch 13/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.1302 - accuracy: 0.0000e+00 - mae: 0.8174 - root_mean_squared_error: 1.0631 - val_loss: 1.1595 - val_accuracy: 0.0000e+00 - val_mae: 0.8149 - val_root_mean_squared_error: 1.0768\n",
            "Epoch 14/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.1198 - accuracy: 0.0000e+00 - mae: 0.8147 - root_mean_squared_error: 1.0582 - val_loss: 1.1508 - val_accuracy: 0.0000e+00 - val_mae: 0.8140 - val_root_mean_squared_error: 1.0727\n",
            "Epoch 15/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.1020 - accuracy: 0.0000e+00 - mae: 0.8110 - root_mean_squared_error: 1.0498 - val_loss: 1.1505 - val_accuracy: 0.0000e+00 - val_mae: 0.8066 - val_root_mean_squared_error: 1.0726\n",
            "Epoch 16/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0908 - accuracy: 0.0000e+00 - mae: 0.8080 - root_mean_squared_error: 1.0444 - val_loss: 1.1303 - val_accuracy: 0.0000e+00 - val_mae: 0.8226 - val_root_mean_squared_error: 1.0631\n",
            "Epoch 17/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0821 - accuracy: 0.0000e+00 - mae: 0.8057 - root_mean_squared_error: 1.0402 - val_loss: 1.1279 - val_accuracy: 0.0000e+00 - val_mae: 0.8109 - val_root_mean_squared_error: 1.0620\n",
            "Epoch 18/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0775 - accuracy: 0.0000e+00 - mae: 0.8041 - root_mean_squared_error: 1.0380 - val_loss: 1.1254 - val_accuracy: 0.0000e+00 - val_mae: 0.8172 - val_root_mean_squared_error: 1.0608\n",
            "Epoch 19/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0732 - accuracy: 0.0000e+00 - mae: 0.8026 - root_mean_squared_error: 1.0360 - val_loss: 1.1232 - val_accuracy: 0.0000e+00 - val_mae: 0.8225 - val_root_mean_squared_error: 1.0598\n",
            "Epoch 20/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0684 - accuracy: 9.3249e-05 - mae: 0.8011 - root_mean_squared_error: 1.0336 - val_loss: 1.1234 - val_accuracy: 0.0000e+00 - val_mae: 0.8195 - val_root_mean_squared_error: 1.0599\n",
            "Epoch 21/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0658 - accuracy: 0.0000e+00 - mae: 0.8002 - root_mean_squared_error: 1.0324 - val_loss: 1.1278 - val_accuracy: 0.0000e+00 - val_mae: 0.8067 - val_root_mean_squared_error: 1.0620\n",
            "Epoch 22/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0622 - accuracy: 0.0000e+00 - mae: 0.7991 - root_mean_squared_error: 1.0306 - val_loss: 1.1214 - val_accuracy: 0.0000e+00 - val_mae: 0.8182 - val_root_mean_squared_error: 1.0590\n",
            "Epoch 23/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0569 - accuracy: 0.0000e+00 - mae: 0.7977 - root_mean_squared_error: 1.0281 - val_loss: 1.1272 - val_accuracy: 0.0000e+00 - val_mae: 0.8036 - val_root_mean_squared_error: 1.0617\n",
            "Epoch 24/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0542 - accuracy: 0.0000e+00 - mae: 0.7967 - root_mean_squared_error: 1.0268 - val_loss: 1.1197 - val_accuracy: 0.0000e+00 - val_mae: 0.8174 - val_root_mean_squared_error: 1.0582\n",
            "Epoch 25/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0495 - accuracy: 0.0000e+00 - mae: 0.7954 - root_mean_squared_error: 1.0244 - val_loss: 1.1313 - val_accuracy: 0.0000e+00 - val_mae: 0.8038 - val_root_mean_squared_error: 1.0636\n",
            "Epoch 26/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0442 - accuracy: 0.0000e+00 - mae: 0.7941 - root_mean_squared_error: 1.0219 - val_loss: 1.1149 - val_accuracy: 0.0000e+00 - val_mae: 0.8136 - val_root_mean_squared_error: 1.0559\n",
            "Epoch 27/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0411 - accuracy: 0.0000e+00 - mae: 0.7931 - root_mean_squared_error: 1.0203 - val_loss: 1.1195 - val_accuracy: 0.0000e+00 - val_mae: 0.8291 - val_root_mean_squared_error: 1.0581\n",
            "Epoch 28/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0392 - accuracy: 0.0000e+00 - mae: 0.7926 - root_mean_squared_error: 1.0194 - val_loss: 1.1174 - val_accuracy: 0.0000e+00 - val_mae: 0.8064 - val_root_mean_squared_error: 1.0571\n",
            "Epoch 29/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0377 - accuracy: 0.0000e+00 - mae: 0.7919 - root_mean_squared_error: 1.0187 - val_loss: 1.1170 - val_accuracy: 0.0000e+00 - val_mae: 0.8122 - val_root_mean_squared_error: 1.0569\n",
            "Epoch 30/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0352 - accuracy: 0.0000e+00 - mae: 0.7913 - root_mean_squared_error: 1.0175 - val_loss: 1.1134 - val_accuracy: 0.0000e+00 - val_mae: 0.8188 - val_root_mean_squared_error: 1.0552\n",
            "Epoch 31/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0325 - accuracy: 0.0000e+00 - mae: 0.7906 - root_mean_squared_error: 1.0161 - val_loss: 1.1133 - val_accuracy: 0.0000e+00 - val_mae: 0.8062 - val_root_mean_squared_error: 1.0551\n",
            "Epoch 32/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0310 - accuracy: 0.0000e+00 - mae: 0.7900 - root_mean_squared_error: 1.0154 - val_loss: 1.1117 - val_accuracy: 0.0000e+00 - val_mae: 0.8145 - val_root_mean_squared_error: 1.0544\n",
            "Epoch 33/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0305 - accuracy: 0.0000e+00 - mae: 0.7898 - root_mean_squared_error: 1.0151 - val_loss: 1.1163 - val_accuracy: 0.0000e+00 - val_mae: 0.8046 - val_root_mean_squared_error: 1.0566\n",
            "Epoch 34/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0299 - accuracy: 9.3249e-05 - mae: 0.7896 - root_mean_squared_error: 1.0148 - val_loss: 1.1148 - val_accuracy: 0.0000e+00 - val_mae: 0.8193 - val_root_mean_squared_error: 1.0558\n",
            "Epoch 35/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0300 - accuracy: 0.0000e+00 - mae: 0.7896 - root_mean_squared_error: 1.0149 - val_loss: 1.1171 - val_accuracy: 0.0000e+00 - val_mae: 0.8288 - val_root_mean_squared_error: 1.0569\n",
            "Epoch 36/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0296 - accuracy: 9.3249e-05 - mae: 0.7895 - root_mean_squared_error: 1.0147 - val_loss: 1.1154 - val_accuracy: 0.0000e+00 - val_mae: 0.8206 - val_root_mean_squared_error: 1.0561\n",
            "Epoch 37/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0288 - accuracy: 9.3249e-05 - mae: 0.7891 - root_mean_squared_error: 1.0143 - val_loss: 1.1284 - val_accuracy: 0.0000e+00 - val_mae: 0.8017 - val_root_mean_squared_error: 1.0622\n",
            "Epoch 38/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0288 - accuracy: 0.0000e+00 - mae: 0.7892 - root_mean_squared_error: 1.0143 - val_loss: 1.1170 - val_accuracy: 0.0000e+00 - val_mae: 0.8098 - val_root_mean_squared_error: 1.0569\n",
            "Epoch 39/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0288 - accuracy: 0.0000e+00 - mae: 0.7891 - root_mean_squared_error: 1.0143 - val_loss: 1.1190 - val_accuracy: 0.0000e+00 - val_mae: 0.8044 - val_root_mean_squared_error: 1.0579\n",
            "Epoch 40/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0288 - accuracy: 0.0000e+00 - mae: 0.7892 - root_mean_squared_error: 1.0143 - val_loss: 1.1169 - val_accuracy: 0.0000e+00 - val_mae: 0.8153 - val_root_mean_squared_error: 1.0568\n",
            "Epoch 41/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0290 - accuracy: 0.0000e+00 - mae: 0.7893 - root_mean_squared_error: 1.0144 - val_loss: 1.1145 - val_accuracy: 0.0000e+00 - val_mae: 0.8144 - val_root_mean_squared_error: 1.0557\n",
            "Epoch 42/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0283 - accuracy: 0.0000e+00 - mae: 0.7891 - root_mean_squared_error: 1.0141 - val_loss: 1.1137 - val_accuracy: 0.0000e+00 - val_mae: 0.8142 - val_root_mean_squared_error: 1.0553\n",
            "Epoch 43/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0276 - accuracy: 0.0000e+00 - mae: 0.7888 - root_mean_squared_error: 1.0137 - val_loss: 1.1207 - val_accuracy: 0.0000e+00 - val_mae: 0.8025 - val_root_mean_squared_error: 1.0586\n",
            "Epoch 44/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0287 - accuracy: 0.0000e+00 - mae: 0.7891 - root_mean_squared_error: 1.0142 - val_loss: 1.1120 - val_accuracy: 0.0000e+00 - val_mae: 0.8108 - val_root_mean_squared_error: 1.0545\n",
            "Epoch 45/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0291 - accuracy: 9.3249e-05 - mae: 0.7894 - root_mean_squared_error: 1.0145 - val_loss: 1.1184 - val_accuracy: 0.0000e+00 - val_mae: 0.8041 - val_root_mean_squared_error: 1.0575\n",
            "Epoch 46/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0299 - accuracy: 0.0000e+00 - mae: 0.7897 - root_mean_squared_error: 1.0148 - val_loss: 1.1176 - val_accuracy: 0.0000e+00 - val_mae: 0.8265 - val_root_mean_squared_error: 1.0572\n",
            "Epoch 47/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0290 - accuracy: 0.0000e+00 - mae: 0.7894 - root_mean_squared_error: 1.0144 - val_loss: 1.1200 - val_accuracy: 0.0000e+00 - val_mae: 0.8177 - val_root_mean_squared_error: 1.0583\n",
            "Epoch 48/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0291 - accuracy: 0.0000e+00 - mae: 0.7895 - root_mean_squared_error: 1.0144 - val_loss: 1.1158 - val_accuracy: 0.0000e+00 - val_mae: 0.8112 - val_root_mean_squared_error: 1.0563\n",
            "Epoch 49/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0301 - accuracy: 0.0000e+00 - mae: 0.7898 - root_mean_squared_error: 1.0150 - val_loss: 1.1179 - val_accuracy: 0.0000e+00 - val_mae: 0.8052 - val_root_mean_squared_error: 1.0573\n",
            "Epoch 50/50\n",
            "1341/1341 [==============================] - 20s 15ms/step - loss: 1.0307 - accuracy: 0.0000e+00 - mae: 0.7900 - root_mean_squared_error: 1.0152 - val_loss: 1.1142 - val_accuracy: 0.0000e+00 - val_mae: 0.8174 - val_root_mean_squared_error: 1.0556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateModel(model,x_test,y_test)"
      ],
      "metadata": {
        "id": "b_zjz2jrR8Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,x_test,y_test = createAutoCF('S-AutoCF',0,1,20,tf.keras.losses.MeanSquaredError(),'adam',0.4,2,16)"
      ],
      "metadata": {
        "id": "BKFMR6ww7xMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateModel(model,x_test,y_test)"
      ],
      "metadata": {
        "id": "TXpgH2C48QBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,x_test,y_test = createAutoCF('S-AutoCF',0,1,20,tf.keras.losses.MeanSquaredError(),'adam',0.4,2,32)"
      ],
      "metadata": {
        "id": "BwKsrdK07zBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateModel(model,x_test,y_test)"
      ],
      "metadata": {
        "id": "W3rhaRmM8Qb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = datapreprocess(df_sample)"
      ],
      "metadata": {
        "id": "c3PUFoU5GrhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genGMFmat(df_sample,df_test,'GMF',0,5,tf.keras.losses.CategoricalCrossentropy(),'adam',256)"
      ],
      "metadata": {
        "id": "fZ9Cm1zW_lDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_s,item_s = FillSparseMat(model,df_test,df_sample)"
      ],
      "metadata": {
        "id": "kwAWuOmaABjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled = Fill_Cf_Matrix(model,user_s,item_s,df_sample['userID'].unique(),df_sample['itemID'].unique())"
      ],
      "metadata": {
        "id": "nuALg9eIArRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled.to_csv(\"/content/Yelp_shopping_GMF_filled_mat.csv\")"
      ],
      "metadata": {
        "id": "3b7mpg60dEsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/Yelp_shopping_GMF_filled_mat.csv\")"
      ],
      "metadata": {
        "id": "5STcxr2W2AKg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}