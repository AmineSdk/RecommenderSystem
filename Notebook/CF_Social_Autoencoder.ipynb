{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CF_Social_Autoencoder.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmineSdk/RecommenderSystem/blob/main/Notebook/CF_Social_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "4fRXByYkGvia",
        "outputId": "f0e94a0d-56b6-4bbb-cfbf-870730f5d52e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle datasets download yelp-shopping-rating-review -f review_shopping.csv\n",
        "# ! kaggle datasets download Yelp-shopping-GMF-filled-mat -f Yelp_shopping_GMF_filled_mat.csv\n",
        "! kaggle datasets download yelp-restaurants-review-rating -f review_restaurant.csv\n",
        "#! kaggle datasets download Aamazon-Cellphones-35k-GMF-filled-mat -f Aamazon_Cellphones_35k_GMF_filled_mat.csv\n",
        "#! kaggle datasets download Yelp-restaurants-GMF-filled-mat -f Yelp_restaurants_GMF_filled_mat.csv"
      ],
      "metadata": {
        "id": "UM0vifCk9CB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d777928e-5e69-489c-dd39-f55668d23f6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading review_restaurant.csv.zip to /content\n",
            " 96% 146M/152M [00:01<00:00, 76.2MB/s]\n",
            "100% 152M/152M [00:01<00:00, 80.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TcGcg4ZzRQD1"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from scipy.sparse import csr_matrix\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import random\n",
        "from keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from progressbar import progressbar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def amazonPreprocess(df):\n",
        "  \n",
        "  df = df[:35000]\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.rename(columns = {'reviewerID':'userID', 'productID' : 'itemID'},inplace = True)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  print(\"DATASET PREPROCESSED\")\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "lAZGdMRODRqE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yelpPreprocess(df):\n",
        "\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  # df.drop(columns=['funny', 'cool','review_id','useful','date'],inplace=True)\n",
        "  # df.rename(columns = {'user_id':'userID', 'business_id':'itemID','stars':'rating'},inplace = True)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  print(\"DATASET PREPROCESSED\")\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "03zm3UHNDu8I"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DatasetToUserItemDataFrame(dataframe,userID,itemID,rating):\n",
        "  #Setting new item IDs from string to int \n",
        "  itemKeys = [] \n",
        "  i = 0\n",
        "  for item in dataframe[itemID].value_counts(sort=False):\n",
        "    temp = np.full((item),i)\n",
        "    itemKeys = np.append(itemKeys,temp)\n",
        "    i += 1\n",
        "\n",
        "  #Setting new user IDs from string to int\n",
        "  userKeysDic = {}\n",
        "  userKeys = np.zeros((dataframe[userID].size))\n",
        "  i = 0\n",
        "  for user in dataframe[userID].unique():\n",
        "    userKeysDic[user] = i\n",
        "    i += 1\n",
        "  i = 0\n",
        "  for user in dataframe[userID]:\n",
        "    userKeys[i] = userKeysDic[user]\n",
        "    i += 1\n",
        "\n",
        "  #Converting arrays from float to int \n",
        "  userKeys = userKeys.astype(int)\n",
        "  itemKeys = itemKeys.astype(int)\n",
        "  print(userKeys.size)\n",
        "  print(itemKeys.size)\n",
        "  \n",
        "  user_item = csr_matrix((dataframe[rating].values.astype(int),(userKeys,itemKeys))) #Creating sparse matrix\n",
        "  user_item_matrix = user_item.toarray() #Converting sparse matrix into array\n",
        "  df_user_item = pd.DataFrame(user_item_matrix,index = dataframe[userID].unique()  ,columns = dataframe[itemID].unique() ) \n",
        "\n",
        "  return df_user_item\n",
        "\n",
        "def getUsersRatings(df,usersList): #returns df_rating containing only users that have friends \n",
        "  temp = df.copy()\n",
        "  for user in list(df['userID']):\n",
        "    if user not in usersList:\n",
        "      temp = temp.drop(temp.loc[temp['userID'] == user].index)\n",
        "  return temp\n",
        "\n",
        "def genIndexColumn(df,min_index,max_index,min_column,max_column): #generates random int index and columns for a given df\n",
        "  New_User_IDs = random.sample(range(min_index,max_index),df.index.size)\n",
        "  New_Item_IDs = random.sample(range(min_column,max_column),df.columns.size)\n",
        "\n",
        "  df_new = pd.DataFrame(df.to_numpy(),index = New_User_IDs,columns = New_Item_IDs)\n",
        "  return df_new\n",
        "\n",
        "def generateIDs(df,index,columns,min_index,max_index,min_column,max_column):\n",
        "  users = df[index].unique()\n",
        "  items = df[columns].unique()\n",
        "  df_train = df.copy()\n",
        "\n",
        "  New_User_IDs = random.sample(range(min_index,max_index),df[index].nunique())\n",
        "  New_Item_IDs = random.sample(range(min_column,max_column),df[columns].nunique())\n",
        "  i = 0\n",
        "  for d in progressbar(users) :\n",
        "    df_train[index].replace({d : New_User_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "\n",
        "  i = 0\n",
        "  for d in progressbar(items) :\n",
        "    df_train[columns].replace({d : New_Item_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "  return df_train\n",
        "\n",
        "def processGmfData(df,index,column,min_user_id,max_user_id,min_item_id,max_item_id):\n",
        "  \n",
        "  \n",
        "  df.rename(columns = {'text': 'reviewText', 'stars': 'rating', 'business_id': 'itemID', 'user_id': 'userID'}, inplace = True)\n",
        "  #df.rename(columns = {'reviewerID':'userID', 'productID' : 'itemID'},inplace = True)\n",
        "  df = generateIDs(df,index,column,min_user_id,max_user_id,min_item_id,max_item_id)\n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "\n",
        "  df['one'] = df['rating'].apply(lambda x: 1 if x==1 else 0)\n",
        "  df['two'] = df['rating'].apply(lambda x: 1 if x==2 else 0)\n",
        "  df['three'] = df['rating'].apply(lambda x: 1 if x==3 else 0)\n",
        "  df['four'] = df['rating'].apply(lambda x: 1 if x==4 else 0)\n",
        "  df['five'] = df['rating'].apply(lambda x: 1 if x==5 else 0)\n",
        "  #df['six'] = df['rating'].apply(lambda x: 1 if x==6 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "08ifZdzgO2u9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GMF\n",
        "def getGMFmodel(num_users,num_items,SIZE_):\n",
        "\n",
        "  input_userID = layers.Input(shape=[1], name='user_ID')\n",
        "  input_itemID = layers.Input(shape=[1], name='item_ID')\n",
        "\n",
        "  user_emb_GMF = layers.Embedding(num_users, SIZE_, name='user_emb_GMF')(input_userID)\n",
        "  item_emb_GMF = layers.Embedding(num_items, SIZE_, name='item_emb_GMF')(input_itemID)\n",
        "\n",
        "  u_GMF = layers.Flatten()(user_emb_GMF)\n",
        "  i_GMF = layers.Flatten()(item_emb_GMF)\n",
        "\n",
        "  dot_layer = layers.Multiply()([u_GMF, i_GMF])\n",
        "\n",
        "  out_layer = layers.Dense(1, activation='sigmoid', name='output')(dot_layer)\n",
        "\n",
        "  GMF = tf.keras.Model([input_userID, input_itemID], out_layer)\n",
        "  \n",
        "  return GMF\n",
        "\n",
        "\n",
        "def user_item_ID_lists(userIDs,itemIDs):\n",
        "  i = 0\n",
        "  item_s = pd.Series()\n",
        "  user_s = pd.Series()\n",
        "\n",
        "  if userIDs.size > itemIDs.size:\n",
        "    for item in progressbar(itemIDs):\n",
        "      temp = pd.Series(userIDs)\n",
        "      user_s = user_s.append(temp)\n",
        "      temp = []\n",
        "      temp = [item for user in userIDs]\n",
        "      temp = pd.Series(temp)\n",
        "      item_s = item_s.append(temp)\n",
        "  else:\n",
        "    for user in progressbar(userIDs):\n",
        "      temp = pd.Series(itemIDs)\n",
        "      item_s = item_s.append(temp)\n",
        "      temp = []\n",
        "      temp = [user for item in itemIDs]\n",
        "      temp = pd.Series(temp)\n",
        "      user_s = user_s.append(temp)\n",
        "\n",
        "  return user_s,item_s\n",
        "\n",
        "\n",
        "def Fill_Cf_Matrix(model,userList,itemList,userIDs,itemIDs):\n",
        "  rowLen = userIDs.size if userIDs.size > itemIDs.size else itemIDs.size\n",
        "\n",
        "  prediction = model.predict([userList,itemList],verbose = 1)\n",
        "  i = 0\n",
        "  row = []\n",
        "  matrix = []\n",
        "  print(\"pred done\")\n",
        "  \n",
        "  for i in progressbar(range(userList.shape[0])):\n",
        "    #result = np.where(prediction[i] == np.amax(prediction[i]))[0][0] + 1\n",
        "    result = prediction[i][0]\n",
        "    row.append(result)\n",
        "    if len(row) == rowLen:\n",
        "      matrix.append(row)\n",
        "      row = []\n",
        "       \n",
        "    i += 1\n",
        "\n",
        "  matrix_arr = np.array(matrix)\n",
        "  if(userIDs.size > itemIDs.size):\n",
        "    matrix_arr = matrix_arr.transpose()\n",
        "  dataframe = pd.DataFrame(matrix_arr, index = userIDs, columns = itemIDs)\n",
        "  \n",
        "  return dataframe\n",
        "\n",
        "def loadDataset(fileID):\n",
        "  dataset = pd.read_csv(DATA[fileID])\n",
        "  print(\"== FILE LOADED ==\")\n",
        "  return dataset\n",
        "\n",
        "def saveDataframe(df,fileID):\n",
        "  file_path = PATH+'GMF_filled_'+DATA[fileID]\n",
        "  df.to_csv(file_path)\n",
        "\n",
        "def genGMFmat(df,df_test,modelID,fileID,nbrE,lossF,OF,emb,filter=None,cb=None):\n",
        "\n",
        "  # df_og = loadDataset(fileID)\n",
        "  # df = processGmfData(df_og,'userID','itemID',1000,2000+df_og['reviewerID'].nunique(),60000,61000+df_og['productID'].nunique())\n",
        "  \n",
        "  x_train ,x_test,y_train,y_test = train_test_split(df_test[['userID','itemID']],df_test['rating'],test_size=0.2)\n",
        "  model_trained = trainModel(modelID,nbrE,lossF,OF,[x_train['userID'],x_train['itemID']],y_train,maxUserID=df_test['userID'].max() + 1,maxItemID =df_test['itemID'].max() + 1,embed_size=emb)\n",
        "  model_trained.evaluate([x_test['userID'],x_test['itemID']],y_test)\n",
        "\n",
        "  #df_mat_filled = FillSparseMat(model_trained,df_test,df)\n",
        "  \n",
        "  return model_trained\n",
        "\n",
        "def FillSparseMat(model,df_prepro,df_og):\n",
        "\n",
        "  user_s,item_s = user_item_ID_lists(df_prepro['userID'].unique(),df_prepro['itemID'].unique())\n",
        "  #df_mat_filled = Fill_Cf_Matrix(model,user_s,item_s,df_og['userID'].unique(),df_og['itemID'].unique())\n",
        "\n",
        "  return user_s,item_s\n",
        "\n",
        "\n",
        "def trainModel(modelID,nbrEpochs,lossF,OF,x_train ,y_train ,mid_layer_ratio=None,nb_layers=None,maxUserID = None,maxItemID = None,embed_size = None,filter_size = None,bs = 16):\n",
        "  \n",
        "  if modelID =='BLCNN':\n",
        "    model = getBLCNNmodel(embed_size,filter_size)\n",
        "    \n",
        "  if modelID == 'GMF':\n",
        "    model = getGMFmodel(maxUserID,maxItemID,embed_size)\n",
        "    print(model.summary())\n",
        "  elif modelID == 'S-AutoCF':\n",
        "    model = getAutoCFmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "    print(model.summary())\n",
        "  elif modelID == 'SS-AutoCF':\n",
        "    model = getSS_HAEmodel(x_train[0],x_train[1],mid_layer_ratio,nb_layers)\n",
        "\n",
        "  model.compile(optimizer = OF,\n",
        "                    loss = lossF,\n",
        "                    metrics= METRICS)\n",
        "  model.fit(x_train,y_train,epochs = nbrEpochs,batch_size = bs,validation_split=0.14)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def getAutoCFmodel(x_train,mid_layer_ratio,nb_layers):\n",
        "  #mid_layer_ratio [0 - 1]\n",
        "  layer_ratio =  mid_layer_ratio + nb_layers*0.05\n",
        "\n",
        "  encoder_input = layers.Input(shape=(x_train.shape[1]),name='user_item')\n",
        "  flat = layers.Flatten()(encoder_input)\n",
        "\n",
        "  if nb_layers != 1:\n",
        "    hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"sigmoid\")(flat)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio -= 0.05\n",
        "      hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"sigmoid\")(hid_encoder)\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*x_train.shape[1],activation=\"sigmoid\")(hid_encoder)\n",
        "  else:\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*x_train.shape[1],activation=\"sigmoid\")(flat)\n",
        "\n",
        "  if nb_layers != 1:\n",
        "    decoder_input = layers.Dense(layer_ratio*x_train.shape[1],activation=\"sigmoid\")(encoder_output)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio += 0.05\n",
        "      decoder_input = layers.Dense(layer_ratio*x_train.shape[1],activation=\"sigmoid\")(decoder_input)\n",
        "    decoder_output = layers.Dense(x_train.shape[1],activation=\"sigmoid\")(decoder_input)\n",
        "  else:\n",
        "    decoder_output = layers.Dense(x_train.shape[1],activation=\"sigmoid\")(encoder_output)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs = encoder_input, outputs = decoder_output)\n",
        "\n",
        "  return autoencoder\n",
        "\n",
        "def getSS_HAEmodel(x_train_rat,x_train_trust,mid_layer_ratio,nb_layers):\n",
        "  \n",
        "  #mid_layer_ratio [0 - 1]\n",
        "  layer_ratio =  mid_layer_ratio + nb_layers*0.1\n",
        "\n",
        "  #Social_Autoencoder\n",
        "  #layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]\n",
        "  rating_input = layers.Input(shape=(x_train_rat.shape[1]),name='user_item')\n",
        "  social_input = layers.Input(shape=(x_train_trust.shape[1]),name='user_user')\n",
        "\n",
        "  flat_rating = layers.Flatten()(rating_input)\n",
        "  flat_social = layers.Flatten()(social_input)\n",
        "\n",
        "  SharedLayer_encoder = layers.Concatenate()([flat_rating,flat_social])\n",
        "\n",
        "  if nb_layers !=1:\n",
        "    hid_encoder = layers.Dense(layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(SharedLayer_encoder)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio -= 0.1\n",
        "      hid_encoder = layers.Dense(layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(hid_encoder)\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(hid_encoder)\n",
        "  else:\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(SharedLayer_encoder)\n",
        "  if nb_layers !=1:\n",
        "    hid_decoder = layers.Dense(layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(encoder_output)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio += 0.1\n",
        "      hid_decoder = layers.Dense(layer_ratio*(x_train_rat.shape[1]+x_train_trust.shape[1]),activation=\"sigmoid\")(hid_decoder)\n",
        "    SharedLayer_decoder =  layers.Dense(x_train_rat.shape[1]+x_train_trust.shape[1],activation=\"sigmoid\")(hid_decoder)\n",
        "  else:\n",
        "    SharedLayer_decoder =  layers.Dense(x_train_rat.shape[1]+x_train_trust.shape[1],activation=\"sigmoid\")(encoder_output)\n",
        "\n",
        "  rating_decoded , social_decoded = tf.split(SharedLayer_decoder,[x_train_rat.shape[1],x_train_trust.shape[1]],1)\n",
        "\n",
        "  # rating_output = layers.Dense(x_train_rat.shape[1],activation=\"relu\",name='rating_output')(SharedLayer_decoder)\n",
        "  # social_output = layers.Dense(x_train_trust.shape[1],activation=\"relu\",name='social_output')(SharedLayer_decoder)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs = [rating_input,social_input], outputs = [rating_decoded,social_decoded])\n",
        "\n",
        "  return autoencoder\n",
        "\n",
        "def createAutoCF(modelID,input_fileID,target_fileID,nbrEpochs,lossF,OF,mid_layer_ratio,nb_layers,bs):\n",
        "\n",
        "  sparseDf = loadDataset(input_fileID)\n",
        "  sparseDf = yelpPreprocess(sparseDf)\n",
        "  df_mat_filled = loadDataset(target_fileID)\n",
        "  df_mat_filled = df_mat_filled.set_index('Unnamed: 0')\n",
        "  df_mat = DatasetToUserItemDataFrame(sparseDf,'userID','itemID','rating')\n",
        "  # if df_mat.shape[0]< df_mat.shape[1]:\n",
        "  #   df_mat = df_mat.T\n",
        "  #   df_mat_filled = df_mat_filled.T\n",
        "  #   print(df_mat_filled.shape,df_mat.shape)\n",
        "  x_train,x_test,y_train,y_test = train_test_split(df_mat,df_mat_filled,test_size = 0.2)\n",
        "  model_trained = trainModel(modelID,nbrEpochs,lossF,OF,x_train,y_train,mid_layer_ratio,nb_layers,bs = bs)\n",
        "\n",
        "  return model_trained , x_test, y_test\n",
        "\n",
        "def createSS_HAE(modelID,input_rating_fileID,target_rating_fileID,trust_fileID,nbrEpochs,lossF,OF,mid_layer_ratio,nb_layers,bs):\n",
        "\n",
        "  sparseDf = loadDataset(input_rating_fileID)\n",
        "  sparseDf = yelpPreprocess(sparseDf)\n",
        "  df_rating_filled = loadDataset(target_rating_fileID)\n",
        "  df_rating_filled = df_rating_filled.set_index('Unnamed: 0')\n",
        "  df_trust_mat = loadDataset(trust_fileID)\n",
        "  df_trust_mat = df_trust_mat.set_index('Unnamed: 0.1')\n",
        "  df_trust_mat.drop(columns=['Unnamed: 0'],inplace=True)\n",
        "  # trust_users_list = list(df_trust_mat.index)\n",
        "  # df_rating_filled = getUsersRatings(df_rating_filled,trust_users_list) \n",
        "  # df_rating_filled = orgDataframe(df_rating_filled,trust_users_list)\n",
        "\n",
        "  #input & target for autoencoder training\n",
        "  df_rating_mat = DatasetToUserItemDataFrame(sparseDf,'userID','itemID','rating')\n",
        "  # df_rating_mat_filled = orgMatDataframe(df_rating_mat_filled,trust_users_list)\n",
        "\n",
        "  #data split\n",
        "  x_rating_train,x_rating_test,y_rating_train,y_rating_test = train_test_split(df_rating_mat,df_rating_filled)\n",
        "  x_train,x_test = train_test_split(df_trust_mat)\n",
        "\n",
        "  model_trained = trainModel(modelID,nbrEpochs,lossF,OF,[x_rating_train,x_train],[y_rating_train,x_train],mid_layer_ratio,nb_layers,bs=bs)\n",
        "\n",
        "  return model_trained , [x_rating_test,y_rating_test] , x_test\n",
        "\n",
        "def evaluateModel(model,x_test,y_test):\n",
        "  model.evaluate(x_test,y_test) \n",
        "\n",
        "def orgDataframe(df_to_org,org_list):\n",
        "\n",
        "  df_org = pd.DataFrame(columns = ['userID','itemID','rating','reviewText'])\n",
        "  for user in org_list:\n",
        "    df_temp = df_to_org[(df_to_org['userID'] == user )]\n",
        "    df_org = pd.concat([df_org,df_temp])\n",
        "\n",
        "  return df_org\n",
        "\n",
        "def orgMatDataframe(df_to_org,org_list):\n",
        "  \n",
        "  df_org = pd.DataFrame(index = list(df_to_org.index),columns = list(df_to_org.columns))\n",
        "  for user in org_list:\n",
        "    df_org.loc[user] = list(df_to_org.loc[user])\n",
        "  \n",
        "  return df_org\n",
        "\n",
        "def getItemsScore(userID,modelID,fileID):\n",
        "  \n",
        "  listeItemScore = pd.DataFrame()\n",
        "  \n",
        "  if modelID == 0:\n",
        "    listItemScore = predSeCF(userID,modelID,fileID)   #CF\n",
        "  elif modelID == 1:\n",
        "    listItemScore = predSeCF(userID,modelID,fileID)   #CF + Sentiment\n",
        "  elif modelID == 2:\n",
        "    listItemScore = predSSeCF(userID,modelID,fileID)  #CF + Social\n",
        "  elif modelID == 3:\n",
        "    listItemScore = predSSeCF(userID,modelID,fileID)  #CF + Sentiment + Social\n",
        "\n",
        "  return listeItemScore \n",
        "\n",
        "def getModel(modelID):\n",
        "  model = tf.keras.models.load_model(MODEL_PATH[modelID])\n",
        "  return model\n",
        "\n",
        "def loadDataFrame(df):\n",
        "  df_loaded = pd.read_csv(DATA[df])\n",
        "  return  df_loaded\n",
        "\n",
        "def predSeCF(userID,modelID,fileID):\n",
        "  \n",
        "  model = getModel(modelID)\n",
        "  df_user_item_mat = pd.read_csv(DATA[fileID])\n",
        "  SparseScoresVec = df_user_item_mat.loc[userID].to_numpy()\n",
        "  listItemScores = df_user_item_mat.loc[userID].to_frame()\n",
        "  listItemScores = listItemScores.reset_index()\n",
        "  listItemScores.set_axis(['itemID','score'],axis='columns',inplace=True)\n",
        "  PredScoresVec = model.predict(SparseScoresVec)\n",
        "  listItemScores.replace(SparseScoresVec,PredScoresVec,inplace = True)\n",
        "\n",
        "  return listItemScores\n",
        "\n",
        "def predSSeCF(userID,modelID,ratings_fileID,trust_fileID):\n",
        "\n",
        "  model = getModel(modelID)\n",
        "  df_user_item_mat = loadDataset(fileID)\n",
        "  df_user_user_mat = pd.read_csv(trust_fileID)\n",
        "  SparseScoresVec = df_user_item_mat.loc[userID].to_numpy()\n",
        "  SparseTrustVec = df_user_user_mat.loc[userID].to_numpy()\n",
        "  listItemScores = df_user_item_mat.loc[userID].to_frame()\n",
        "  listItemScores = listItemScores.reset_index()\n",
        "  listItemScores.set_axis(['itemID','score'],axis='columns',inplace=True)\n",
        "  PredScoresVec,SparseTrustVec = model.predict([SparseScoresVec,SparseTrustVec])\n",
        "  listItemScores.replace(SparseScoresVec,PredScoresVec,inplace = True)\n",
        "\n",
        "  return listItemScores\n",
        "  "
      ],
      "metadata": {
        "id": "z6KHUFF9gwT5"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datapreprocess(df_sample):\n",
        "  \n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df_sample = generateIDs(df_sample,'userID','itemID',5000,6000+df_sample['userID'].nunique(),8000,8000+df_sample['itemID'].nunique())\n",
        "  \n",
        "  df_sample['one'] = df_sample['rating'].apply(lambda x: 1 if x==1 else 0)\n",
        "  df_sample['two'] = df_sample['rating'].apply(lambda x: 1 if x==2 else 0)\n",
        "  df_sample['three'] = df_sample['rating'].apply(lambda x: 1 if x==3 else 0)\n",
        "  df_sample['four'] = df_sample['rating'].apply(lambda x: 1 if x==4 else 0)\n",
        "  df_sample['five'] = df_sample['rating'].apply(lambda x: 1 if x==5 else 0)\n",
        "  #df['six'] = df['rating'].apply(lambda x: 1 if x==6 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df_sample"
      ],
      "metadata": {
        "id": "EHC86cO8GJcK"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import preprocessing\n",
        "from sklearn.compose import ColumnTransformer"
      ],
      "metadata": {
        "id": "aLQUQj6_Mh5m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(df):\n",
        "  transformer = ColumnTransformer(transformers=[('norm', preprocessing.MinMaxScaler(feature_range=(0, 1)), ['rating'])])\n",
        "  col = transformer.fit_transform(df)\n",
        "  return col"
      ],
      "metadata": {
        "id": "SPUgPW_LMXYI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.to_csv(\"/content/review_restaurant_norm.csv\")"
      ],
      "metadata": {
        "id": "XfvWR0oftsKh"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-ranking"
      ],
      "metadata": {
        "id": "TaV8wBuzlCtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_ranking as tfr"
      ],
      "metadata": {
        "id": "TZ7BFJ0HlG0P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = ['/content/review_restaurant_norm.csv','/content/Yelp_restaurants_GMF_filled_mat.csv','/content/gdrive/MyDrive/RESTAURANT_Final_UserUser_trust_DF.csv','/content/Yelp_shopping_GMF_filled_mat.csv']\n",
        "MODEL_PATH = []\n",
        "PATH = ''\n",
        "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "precision = tf.keras.metrics.Precision()\n",
        "# ndcg = tfr.keras.metrics.NDCGMetric()\n",
        "METRICS = ['mae',rmse]"
      ],
      "metadata": {
        "id": "3eS1ZsrzFU-m"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/review_restaurant_norm.csv')"
      ],
      "metadata": {
        "id": "BUPRwF2FBGWZ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = yelpPreprocess(df)"
      ],
      "metadata": {
        "id": "VvOIegEu5jr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,x_test,y_test = createAutoCF('S-AutoCF',0,1,10,tf.keras.losses.MeanSquaredError(),'adam',0.3,1,8)"
      ],
      "metadata": {
        "id": "jWu3NfsAGS2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluatem"
      ],
      "metadata": {
        "id": "BKfbmFoUI7yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,[x_rating_test,y_rating_test],y_test = createSS_HAE('SS-AutoCF',0,1,2,10,tf.keras.losses.MeanSquaredError(),'adam',0.3,1,8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muCu7AN1tOAy",
        "outputId": "ec8c9fa9-de27-4f32-e1f3-7292a8c8c6c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== FILE LOADED ==\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py:311: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  return func(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATASET PREPROCESSED\n",
            "== FILE LOADED ==\n",
            "== FILE LOADED ==\n",
            "356705\n",
            "356705\n",
            "Epoch 1/10\n",
            "351/351 [==============================] - 22s 62ms/step - loss: 0.0193 - tf.split_loss: 0.0144 - tf.split_1_loss: 0.0049 - tf.split_mae: 0.0914 - tf.split_root_mean_squared_error: 0.1200 - tf.split_1_mae: 0.0406 - tf.split_1_root_mean_squared_error: 0.0702 - val_loss: 0.0160 - val_tf.split_loss: 0.0130 - val_tf.split_1_loss: 0.0030 - val_tf.split_mae: 0.0871 - val_tf.split_root_mean_squared_error: 0.1142 - val_tf.split_1_mae: 0.0309 - val_tf.split_1_root_mean_squared_error: 0.0549\n",
            "Epoch 2/10\n",
            "351/351 [==============================] - 21s 61ms/step - loss: 0.0181 - tf.split_loss: 0.0154 - tf.split_1_loss: 0.0027 - tf.split_mae: 0.0926 - tf.split_root_mean_squared_error: 0.1241 - tf.split_1_mae: 0.0289 - tf.split_1_root_mean_squared_error: 0.0517 - val_loss: 0.0167 - val_tf.split_loss: 0.0144 - val_tf.split_1_loss: 0.0023 - val_tf.split_mae: 0.0904 - val_tf.split_root_mean_squared_error: 0.1201 - val_tf.split_1_mae: 0.0260 - val_tf.split_1_root_mean_squared_error: 0.0476\n",
            "Epoch 3/10\n",
            "351/351 [==============================] - 22s 64ms/step - loss: 0.0166 - tf.split_loss: 0.0145 - tf.split_1_loss: 0.0021 - tf.split_mae: 0.0905 - tf.split_root_mean_squared_error: 0.1205 - tf.split_1_mae: 0.0250 - tf.split_1_root_mean_squared_error: 0.0460 - val_loss: 0.0160 - val_tf.split_loss: 0.0140 - val_tf.split_1_loss: 0.0020 - val_tf.split_mae: 0.0892 - val_tf.split_root_mean_squared_error: 0.1183 - val_tf.split_1_mae: 0.0235 - val_tf.split_1_root_mean_squared_error: 0.0447\n",
            "Epoch 4/10\n",
            "351/351 [==============================] - 23s 65ms/step - loss: 0.0160 - tf.split_loss: 0.0141 - tf.split_1_loss: 0.0019 - tf.split_mae: 0.0891 - tf.split_root_mean_squared_error: 0.1187 - tf.split_1_mae: 0.0232 - tf.split_1_root_mean_squared_error: 0.0433 - val_loss: 0.0156 - val_tf.split_loss: 0.0137 - val_tf.split_1_loss: 0.0019 - val_tf.split_mae: 0.0891 - val_tf.split_root_mean_squared_error: 0.1172 - val_tf.split_1_mae: 0.0226 - val_tf.split_1_root_mean_squared_error: 0.0430\n",
            "Epoch 5/10\n",
            "351/351 [==============================] - 22s 62ms/step - loss: 0.0147 - tf.split_loss: 0.0130 - tf.split_1_loss: 0.0017 - tf.split_mae: 0.0863 - tf.split_root_mean_squared_error: 0.1140 - tf.split_1_mae: 0.0220 - tf.split_1_root_mean_squared_error: 0.0414 - val_loss: 0.0153 - val_tf.split_loss: 0.0135 - val_tf.split_1_loss: 0.0017 - val_tf.split_mae: 0.0878 - val_tf.split_root_mean_squared_error: 0.1164 - val_tf.split_1_mae: 0.0213 - val_tf.split_1_root_mean_squared_error: 0.0414\n",
            "Epoch 6/10\n",
            "351/351 [==============================] - 22s 62ms/step - loss: 0.0134 - tf.split_loss: 0.0118 - tf.split_1_loss: 0.0016 - tf.split_mae: 0.0829 - tf.split_root_mean_squared_error: 0.1086 - tf.split_1_mae: 0.0213 - tf.split_1_root_mean_squared_error: 0.0400 - val_loss: 0.0157 - val_tf.split_loss: 0.0141 - val_tf.split_1_loss: 0.0016 - val_tf.split_mae: 0.0899 - val_tf.split_root_mean_squared_error: 0.1187 - val_tf.split_1_mae: 0.0208 - val_tf.split_1_root_mean_squared_error: 0.0405\n",
            "Epoch 7/10\n",
            "351/351 [==============================] - 21s 61ms/step - loss: 0.0123 - tf.split_loss: 0.0108 - tf.split_1_loss: 0.0015 - tf.split_mae: 0.0796 - tf.split_root_mean_squared_error: 0.1040 - tf.split_1_mae: 0.0206 - tf.split_1_root_mean_squared_error: 0.0388 - val_loss: 0.0149 - val_tf.split_loss: 0.0133 - val_tf.split_1_loss: 0.0016 - val_tf.split_mae: 0.0880 - val_tf.split_root_mean_squared_error: 0.1152 - val_tf.split_1_mae: 0.0202 - val_tf.split_1_root_mean_squared_error: 0.0397\n",
            "Epoch 8/10\n",
            "351/351 [==============================] - 22s 61ms/step - loss: 0.0108 - tf.split_loss: 0.0094 - tf.split_1_loss: 0.0014 - tf.split_mae: 0.0746 - tf.split_root_mean_squared_error: 0.0970 - tf.split_1_mae: 0.0200 - tf.split_1_root_mean_squared_error: 0.0378 - val_loss: 0.0145 - val_tf.split_loss: 0.0130 - val_tf.split_1_loss: 0.0015 - val_tf.split_mae: 0.0874 - val_tf.split_root_mean_squared_error: 0.1140 - val_tf.split_1_mae: 0.0199 - val_tf.split_1_root_mean_squared_error: 0.0393\n",
            "Epoch 9/10\n",
            "351/351 [==============================] - 22s 62ms/step - loss: 0.0099 - tf.split_loss: 0.0086 - tf.split_1_loss: 0.0014 - tf.split_mae: 0.0711 - tf.split_root_mean_squared_error: 0.0925 - tf.split_1_mae: 0.0199 - tf.split_1_root_mean_squared_error: 0.0372 - val_loss: 0.0149 - val_tf.split_loss: 0.0134 - val_tf.split_1_loss: 0.0015 - val_tf.split_mae: 0.0888 - val_tf.split_root_mean_squared_error: 0.1156 - val_tf.split_1_mae: 0.0204 - val_tf.split_1_root_mean_squared_error: 0.0392\n",
            "Epoch 10/10\n",
            "351/351 [==============================] - 21s 61ms/step - loss: 0.0089 - tf.split_loss: 0.0075 - tf.split_1_loss: 0.0013 - tf.split_mae: 0.0667 - tf.split_root_mean_squared_error: 0.0867 - tf.split_1_mae: 0.0196 - tf.split_1_root_mean_squared_error: 0.0365 - val_loss: 0.0142 - val_tf.split_loss: 0.0128 - val_tf.split_1_loss: 0.0015 - val_tf.split_mae: 0.0867 - val_tf.split_root_mean_squared_error: 0.1129 - val_tf.split_1_mae: 0.0198 - val_tf.split_1_root_mean_squared_error: 0.0385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateModel(model,[x_rating_test,y_test],[y_rating_test,y_test])"
      ],
      "metadata": {
        "id": "6Yu8xuuKnpD0",
        "outputId": "05e0a616-7c42-4e14-b907-a276716e7a7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34/34 [==============================] - 1s 22ms/step - loss: 0.0145 - tf.split_loss: 0.0129 - tf.split_1_loss: 0.0016 - tf.split_mae: 0.0872 - tf.split_root_mean_squared_error: 0.1137 - tf.split_1_mae: 0.0199 - tf.split_1_root_mean_squared_error: 0.0395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/gdrive/MyDrive/CF\")"
      ],
      "metadata": {
        "id": "u0_xGTElIBK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(4):\n",
        "  print(i)"
      ],
      "metadata": {
        "id": "KXeCyJJORJhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_test = model.predict(x_test)\n",
        "y_test_array = np.array(y_test)\n",
        "# precision_10 = tf.keras.metrics.Precision(top_k =10)\n",
        "ndcg10 = tfr.keras.metrics.NDCGMetric(topn=10)\n",
        "ndcg10.update_state(y_test_array,predicted_test)\n",
        "# precision_10.update_state(y_test_array,predicted_test)\n",
        "ndcg10.result().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6RXLSSCyNSD",
        "outputId": "36f69be8-acb6-4cd4-93b1-ec9f42075a95"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.45489812"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = pd.read_csv(\"/content/review_shopping.csv.zip\")"
      ],
      "metadata": {
        "id": "YFf76MumHofs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.rename(columns = {'text': 'reviewText', 'stars': 'rating', 'business_id': 'itemID', 'user_id': 'userID'}, inplace = True)\n",
        "df_sample.dropna(how='any',axis=0)\n",
        "df_sample.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)"
      ],
      "metadata": {
        "id": "mvGZ7ZP-IGG-"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.head()"
      ],
      "metadata": {
        "id": "ZnJVhwlYIluq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = datapreprocess(df_sample)"
      ],
      "metadata": {
        "id": "c3PUFoU5GrhR",
        "outputId": "9a3497f9-cf92-4eda-92f9-032d165d1b91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "== PREPROCESSING DATA ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (4346 of 4346) |####################| Elapsed Time: 0:02:07 Time:  0:02:07\n",
            "100% (15588 of 15588) |##################| Elapsed Time: 0:14:52 Time:  0:14:52\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== DATA PREPROCESSED ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "metadata": {
        "id": "44G4_w6pgRAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = genGMFmat(df_sample,df_test,'GMF',0,2,tf.keras.losses.MeanSquaredError(),'adam',32)"
      ],
      "metadata": {
        "id": "fZ9Cm1zW_lDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_s,item_s = FillSparseMat(model,df_test,df_sample)"
      ],
      "metadata": {
        "id": "kwAWuOmaABjb",
        "outputId": "04f396c2-3c47-44ba-a6fe-0f04a0c6de90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "100% (4346 of 4346) |####################| Elapsed Time: 0:27:32 Time:  0:27:32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled = Fill_Cf_Matrix(model,user_s,item_s,df_sample['userID'].unique(),df_sample['itemID'].unique())"
      ],
      "metadata": {
        "id": "nuALg9eIArRd",
        "outputId": "438c5428-9e24-4ae4-f533-13e9419fef45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2117046/2117046 [==============================] - 2817s 1ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0% (58217 of 67745448) |               | Elapsed Time: 0:00:00 ETA:   0:03:52"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (67745448 of 67745448) |############| Elapsed Time: 0:03:48 Time:  0:03:48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled.to_csv(\"/content/Yelp_shopping_GMF_filled_mat.csv\")"
      ],
      "metadata": {
        "id": "3b7mpg60dEsv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/Yelp_shopping_GMF_filled_mat.csv\")"
      ],
      "metadata": {
        "id": "5STcxr2W2AKg",
        "outputId": "8d434b33-0b7c-4ac9-bdaa-eeae48733183",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ef4718b7-ab27-496e-9e6d-0f92cc4dde46\", \"Yelp_shopping_GMF_filled_mat.csv\", 702280977)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}