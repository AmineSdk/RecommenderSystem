{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CF_Social_Autoencoder.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmineSdk/RecommenderSystem/blob/main/Notebook/CF_Social_Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "4fRXByYkGvia",
        "outputId": "c5524af6-cf13-4106-a0fe-5d84ae4fdc83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download yelp-shopping-rating-review -f review_shopping.csv\n",
        "# ! kaggle datasets download Yelp-shopping-GMF-filled-mat -f Yelp_shopping_GMF_filled_mat.csv\n",
        "# ! kaggle datasets download yelp-shopping-rating-review -f review_shopping.csv\n",
        "#! kaggle datasets download Aamazon-Cellphones-35k-GMF-filled-mat -f Aamazon_Cellphones_35k_GMF_filled_mat.csv\n",
        "# ! kaggle datasets download Yelp-restaurant-GMF-filled-mat -f Yelp_restaurant_GMF_filled_mat.csv"
      ],
      "metadata": {
        "id": "UM0vifCk9CB0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2b43ccd-19d8-4047-81fc-e9f821d524c6"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Downloading review_shopping.csv.zip to /content\n",
            " 39% 9.00M/22.9M [00:00<00:00, 14.9MB/s]\n",
            "100% 22.9M/22.9M [00:00<00:00, 33.6MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TcGcg4ZzRQD1"
      },
      "outputs": [],
      "source": [
        "from sklearn import preprocessing\n",
        "from scipy.sparse import csr_matrix\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import pandas as pd\n",
        "import random\n",
        "from keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from progressbar import progressbar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def amazonPreprocess(df):\n",
        "  \n",
        "  df = df[:35000]\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.rename(columns = {'reviewerID':'userID', 'productID' : 'itemID'},inplace = True)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  print(\"DATASET PREPROCESSED\")\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "lAZGdMRODRqE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def yelpPreprocess(df):\n",
        "\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.drop(columns=['funny', 'cool','review_id','useful','date'],inplace=True)\n",
        "  df.rename(columns = {'user_id':'userID', 'business_id':'itemID','stars':'rating'},inplace = True)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "  print(\"DATASET PREPROCESSED\")\n",
        "\n",
        "  return df\n"
      ],
      "metadata": {
        "id": "03zm3UHNDu8I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DatasetToUserItemDataFrame(dataframe,userID,itemID,rating):\n",
        "  #Setting new item IDs from string to int \n",
        "  itemKeys = [] \n",
        "  i = 0\n",
        "  for item in dataframe[itemID].value_counts(sort=False):\n",
        "    temp = np.full((item),i)\n",
        "    itemKeys = np.append(itemKeys,temp)\n",
        "    i += 1\n",
        "\n",
        "  #Setting new user IDs from string to int\n",
        "  userKeysDic = {}\n",
        "  userKeys = np.zeros((dataframe[userID].size))\n",
        "  i = 0\n",
        "  for user in dataframe[userID].unique():\n",
        "    userKeysDic[user] = i\n",
        "    i += 1\n",
        "  i = 0\n",
        "  for user in dataframe[userID]:\n",
        "    userKeys[i] = userKeysDic[user]\n",
        "    i += 1\n",
        "\n",
        "  #Converting arrays from float to int \n",
        "  userKeys = userKeys.astype(int)\n",
        "  itemKeys = itemKeys.astype(int)\n",
        "\n",
        "  \n",
        "  user_item = csr_matrix((dataframe[rating].values.astype(int),(userKeys,itemKeys))) #Creating sparse matrix\n",
        "  user_item_matrix = user_item.toarray() #Converting sparse matrix into array\n",
        "  df_user_item = pd.DataFrame(user_item_matrix,index = dataframe[userID].unique()  ,columns = dataframe[itemID].unique() ) \n",
        "\n",
        "  return df_user_item\n",
        "\n",
        "def getUsersRatings(df,usersList): #returns df_rating containing only users that have friends \n",
        "  temp = df.copy()\n",
        "  for user in list(df['userID']):\n",
        "    if user not in usersList:\n",
        "      temp = temp.drop(temp.loc[temp['userID'] == user].index)\n",
        "  return temp\n",
        "\n",
        "def genIndexColumn(df,min_index,max_index,min_column,max_column): #generates random int index and columns for a given df\n",
        "  New_User_IDs = random.sample(range(min_index,max_index),df.index.size)\n",
        "  New_Item_IDs = random.sample(range(min_column,max_column),df.columns.size)\n",
        "\n",
        "  df_new = pd.DataFrame(df.to_numpy(),index = New_User_IDs,columns = New_Item_IDs)\n",
        "  return df_new\n",
        "\n",
        "def generateIDs(df,index,columns,min_index,max_index,min_column,max_column):\n",
        "  users = df[index].unique()\n",
        "  items = df[columns].unique()\n",
        "  df_train = df.copy()\n",
        "\n",
        "  New_User_IDs = random.sample(range(min_index,max_index),df[index].nunique())\n",
        "  New_Item_IDs = random.sample(range(min_column,max_column),df[columns].nunique())\n",
        "  i = 0\n",
        "  for d in progressbar(users) :\n",
        "    df_train[index].replace({d : New_User_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "\n",
        "  i = 0\n",
        "  for d in progressbar(items) :\n",
        "    df_train[columns].replace({d : New_Item_IDs[i]}, inplace=True)\n",
        "    i+=1\n",
        "  return df_train\n",
        "\n",
        "def processGmfData(df,index,column,min_user_id,max_user_id,min_item_id,max_item_id):\n",
        "  \n",
        "  \n",
        "  df.rename(columns = {'text': 'reviewText', 'stars': 'rating', 'business_id': 'itemID', 'user_id': 'userID'}, inplace = True)\n",
        "  #df.rename(columns = {'reviewerID':'userID', 'productID' : 'itemID'},inplace = True)\n",
        "  df = generateIDs(df,index,column,min_user_id,max_user_id,min_item_id,max_item_id)\n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)\n",
        "\n",
        "  df['one'] = df['rating'].apply(lambda x: 1 if x==1 else 0)\n",
        "  df['two'] = df['rating'].apply(lambda x: 1 if x==2 else 0)\n",
        "  df['three'] = df['rating'].apply(lambda x: 1 if x==3 else 0)\n",
        "  df['four'] = df['rating'].apply(lambda x: 1 if x==4 else 0)\n",
        "  df['five'] = df['rating'].apply(lambda x: 1 if x==5 else 0)\n",
        "  #df['six'] = df['rating'].apply(lambda x: 1 if x==6 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "08ifZdzgO2u9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#GMF\n",
        "def getGMFmodel(num_users,num_items,SIZE_):\n",
        "\n",
        "  input_userID = layers.Input(shape=[1], name='user_ID')\n",
        "  input_itemID = layers.Input(shape=[1], name='item_ID')\n",
        "\n",
        "  user_emb_GMF = layers.Embedding(num_users, SIZE_, name='user_emb_GMF')(input_userID)\n",
        "  item_emb_GMF = layers.Embedding(num_items, SIZE_, name='item_emb_GMF')(input_itemID)\n",
        "\n",
        "  u_GMF = layers.Flatten()(user_emb_GMF)\n",
        "  i_GMF = layers.Flatten()(item_emb_GMF)\n",
        "\n",
        "  dot_layer = layers.Multiply()([u_GMF, i_GMF])\n",
        "\n",
        "  out_layer = layers.Dense(1, activation='relu', name='output')(dot_layer)\n",
        "\n",
        "  GMF = tf.keras.Model([input_userID, input_itemID], out_layer)\n",
        "  \n",
        "  return GMF\n",
        "\n",
        "\n",
        "def user_item_ID_lists(userIDs,itemIDs):\n",
        "  i = 0\n",
        "  item_s = pd.Series()\n",
        "  user_s = pd.Series()\n",
        "\n",
        "  if userIDs.size > itemIDs.size:\n",
        "    for item in progressbar(itemIDs):\n",
        "      temp = pd.Series(userIDs)\n",
        "      user_s = user_s.append(temp)\n",
        "      temp = []\n",
        "      temp = [item for user in userIDs]\n",
        "      temp = pd.Series(temp)\n",
        "      item_s = item_s.append(temp)\n",
        "  else:\n",
        "    for user in progressbar(userIDs):\n",
        "      temp = pd.Series(itemIDs)\n",
        "      item_s = item_s.append(temp)\n",
        "      temp = []\n",
        "      temp = [user for item in itemIDs]\n",
        "      temp = pd.Series(temp)\n",
        "      user_s = user_s.append(temp)\n",
        "\n",
        "  return user_s,item_s\n",
        "\n",
        "\n",
        "def Fill_Cf_Matrix(model,userList,itemList,userIDs,itemIDs):\n",
        "  rowLen = userIDs.size if userIDs.size > itemIDs.size else itemIDs.size\n",
        "\n",
        "  prediction = model.predict([userList,itemList],verbose = 1)\n",
        "  i = 0\n",
        "  row = []\n",
        "  matrix = []\n",
        "  print(\"pred done\")\n",
        "  \n",
        "  for i in progressbar(range(userList.shape[0])):\n",
        "    result = np.where(prediction[i] == np.amax(prediction[i]))[0][0] + 1\n",
        "    row.append(result)\n",
        "    if len(row) == rowLen:\n",
        "      matrix.append(row)\n",
        "      row = []\n",
        "       \n",
        "    i += 1\n",
        "\n",
        "  matrix_arr = np.array(matrix)\n",
        "  if(userIDs.size > itemIDs.size):\n",
        "    matrix_arr = matrix_arr.transpose()\n",
        "  dataframe = pd.DataFrame(matrix_arr, index = userIDs, columns = itemIDs)\n",
        "  \n",
        "  return dataframe\n",
        "\n",
        "def loadDataset(fileID):\n",
        "  dataset = pd.read_csv(DATA[fileID])\n",
        "  print(\"== FILE LOADED ==\")\n",
        "  return dataset\n",
        "\n",
        "def saveDataframe(df,fileID):\n",
        "  file_path = PATH+'GMF_filled_'+DATA[fileID]\n",
        "  df.to_csv(file_path)\n",
        "\n",
        "def genGMFmat(df,df_test,modelID,fileID,nbrE,lossF,OF,emb,filter=None,cb=None):\n",
        "\n",
        "  # df_og = loadDataset(fileID)\n",
        "  # df = processGmfData(df_og,'userID','itemID',1000,2000+df_og['reviewerID'].nunique(),60000,61000+df_og['productID'].nunique())\n",
        "  \n",
        "  x_train ,x_test,y_train,y_test = train_test_split(df_test[['userID','itemID']],df_test['rating'],test_size=0.2)\n",
        "  model_trained = trainModel(modelID,nbrE,lossF,OF,[x_train['userID'],x_train['itemID']],y_train,maxUserID=df_test['userID'].max() + 1,maxItemID =df_test['itemID'].max() + 1,embed_size=emb)\n",
        "  model_trained.evaluate([x_test['userID'],x_test['itemID']],y_test)\n",
        "\n",
        "  #df_mat_filled = FillSparseMat(model_trained,df_test,df)\n",
        "  \n",
        "  return model_trained\n",
        "\n",
        "def FillSparseMat(model,df_prepro,df_og):\n",
        "\n",
        "  user_s,item_s = user_item_ID_lists(df_prepro['userID'].unique(),df_prepro['itemID'].unique())\n",
        "  #df_mat_filled = Fill_Cf_Matrix(model,user_s,item_s,df_og['userID'].unique(),df_og['itemID'].unique())\n",
        "\n",
        "  return user_s,item_s\n",
        "\n",
        "\n",
        "def trainModel(modelID,nbrEpochs,lossF,OF,x_train ,y_train ,mid_layer_ratio=None,nb_layers=None,maxUserID = None,maxItemID = None,embed_size = None,filter_size = None,bs = 8):\n",
        "  \n",
        "  if modelID =='BLCNN':\n",
        "    model = getBLCNNmodel(embed_size,filter_size)\n",
        "    \n",
        "  if modelID == 'GMF':\n",
        "    model = getGMFmodel(maxUserID,maxItemID,embed_size)\n",
        "    print(model.summary())\n",
        "  elif modelID == 'S-AutoCF':\n",
        "    model = getAutoCFmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "    print(model.summary())\n",
        "  elif modelID == 'SS-AutoCF':\n",
        "    model = getSS_HAEmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "\n",
        "  model.compile(optimizer = OF,\n",
        "                    loss = lossF,\n",
        "                    metrics= METRICS)\n",
        "  model.fit(x_train,y_train,epochs = nbrEpochs,batch_size = bs,validation_split=0.14)\n",
        "  \n",
        "  return model\n",
        "\n",
        "def getAutoCFmodel(x_train,mid_layer_ratio,nb_layers):\n",
        "  #mid_layer_ratio [0 - 1]\n",
        "  layer_ratio =  mid_layer_ratio + nb_layers*0.05\n",
        "\n",
        "  encoder_input = layers.Input(shape=(x_train.shape[1]),name='user_item')\n",
        "  flat = layers.Flatten()(encoder_input)\n",
        "\n",
        "  if nb_layers != 1:\n",
        "    hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(flat)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio -= 0.05\n",
        "      hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(hid_encoder)\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*x_train.shape[1],activation=\"relu\")(hid_encoder)\n",
        "  else:\n",
        "    encoder_output = layers.Dense(mid_layer_ratio*x_train.shape[1],activation=\"relu\")(flat)\n",
        "\n",
        "  if nb_layers != 1:\n",
        "    decoder_input = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(encoder_output)\n",
        "    for i in range(nb_layers-1):\n",
        "      layer_ratio += 0.05\n",
        "      decoder_input = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(decoder_input)\n",
        "    decoder_output = layers.Dense(x_train.shape[1],activation=\"relu\")(decoder_input)\n",
        "  else:\n",
        "    decoder_output = layers.Dense(x_train.shape[1],activation=\"relu\")(encoder_output)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs = encoder_input, outputs = decoder_output)\n",
        "\n",
        "  return autoencoder\n",
        "\n",
        "def getSS_HAEmodel(x_train,mid_layer_ratio,nb_layers):\n",
        "  \n",
        "  #mid_layer_ratio [0 - 1]\n",
        "  layer_ratio =  mid_layer_ratio + nb_layers*0.1\n",
        "\n",
        "  #Social_Autoencoder\n",
        "\n",
        "  rating_input = layers.Input(shape=(x_train.shape[1]),name='user_item')\n",
        "  social_input = layers.Input(shape=(x_train.shape[1]),name='user_user')\n",
        "\n",
        "  flat_rating = layers.Flatten()(rating_input)\n",
        "  flat_social = layers.Flatten()(social_input)\n",
        "\n",
        "  #dropout = layers.Dropout(.2)(flat)\n",
        "  SharedLayer_encoder = layers.Concatenate()([flat_rating,flat_social])\n",
        "  for i in range(nb_layers):\n",
        "    hid_encoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(hid_encoder)\n",
        "    layer_ratio -= 0.1\n",
        "  hid_encoder(SharedLayer_encoder)\n",
        "  encoder_output = layers.Dense(mid_layer_ratio,activation=\"relu\")(hid_encoder)\n",
        "  for i in range(nb_layers):\n",
        "    hid_decoder = layers.Dense(layer_ratio*x_train.shape[1],activation=\"relu\")(hid_decoder)\n",
        "    layer_ratio += 0.1\n",
        "  hid_decoder(encoder_output)\n",
        "  SharedLayer_decoder =  layers.Dense(df_mat_rating.shape[1]+df_mat_trust.shape[1],activation=\"relu\")(hid_decoder)\n",
        "  rating_decoded , social_decoded = tf.split(SharedLayer_decoder,[df_mat_rating.shape[1],df_mat_trust.shape[1]],1)\n",
        "\n",
        "  rating_output = layers.Dense(df_mat_rating.shape[1],activation=\"relu\",name='rating_output')(rating_decoded)\n",
        "  social_output = layers.Dense(df_mat_trust.shape[1],activation=\"relu\",name='social_output')(social_decoded)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs = [rating_input,social_input], outputs = [rating_output,social_output])\n",
        "\n",
        "  return autoencoder\n",
        "\n",
        "def createAutoCF(modelID,input_fileID,target_fileID,nbrEpochs,lossF,OF,mid_layer_ratio,nb_layers,bs):\n",
        "\n",
        "  sparseDf = loadDataset(input_fileID)\n",
        "  sparseDf = yelpPreprocess(sparseDf)\n",
        "  df_mat_filled = loadDataset(target_fileID)\n",
        "  df_mat_filled = df_mat_filled.set_index('Unnamed: 0')\n",
        "  df_mat = DatasetToUserItemDataFrame(sparseDf,'userID','itemID','rating')\n",
        "  if df_mat.shape[0]< df_mat.shape[1]:\n",
        "    df_mat = df_mat.T\n",
        "    df_mat_filled = df_mat_filled.T\n",
        "    print(df_mat_filled.shape,df_mat.shape)\n",
        "  x_train,x_test,y_train,y_test = train_test_split(df_mat,df_mat_filled,test_size = 0.2)\n",
        "  model_trained = trainModel(modelID,nbrEpochs,lossF,OF,x_train,y_train,mid_layer_ratio,nb_layers,bs = bs)\n",
        "\n",
        "  return model_trained , x_test, y_test\n",
        "\n",
        "def createSS_HAE(modelID,input_rating_fileID,target_rating_fileID,trust_fileID,nbrEpochs,lossF,OF,mid_layer_ratio,nb_layers):\n",
        "\n",
        "  sparseDf = loadDataset(input_rating_fileID)\n",
        "  df_rating_filled = loadDataset(target_rating_fileID)\n",
        "  df_trust_mat = loadDataset(trust_fileID)\n",
        "  trust_users_list = list(df_trust_mat.index)\n",
        "  df_rating_filled = getUsersRatings(df_rating_filled,trust_users_list) \n",
        "  df_rating_filled = orgDataframe(df_rating_filled,trust_users_list)\n",
        "\n",
        "  #input & target for autoencoder training\n",
        "  df_rating_mat = DatasetToUserItemDataFrame(sparseDf,'userID','itemID','rating')\n",
        "  df_rating_mat_filled = orgMatDataframe(df_rating_mat_filled,trust_users_list)\n",
        "\n",
        "  #data split\n",
        "  x_rating_train,x_rating_test,y_rating_train,y_rating_test = train_test_split(df_rating_mat,df_rating_mat_filled)\n",
        "  x_train,x_test = train_test_split(df_trust_mat)\n",
        "\n",
        "  model_trained = trainModel(modelID,nbrEpochs,lossF,OF,[x_rating_train,x_train],[y_rating_train,x_train],mid_layer_ratio,nb_layers)\n",
        "\n",
        "  return model_trained , [x_rating_test,y_rating_test] , x_test\n",
        "\n",
        "def evaluateModel(model,x_test,y_test):\n",
        "  model.evaluate(x_test,y_test) \n",
        "\n",
        "def orgDataframe(df_to_org,org_list):\n",
        "\n",
        "  df_org = pd.DataFrame(columns = ['userID','itemID','rating','reviewText'])\n",
        "  for user in org_list:\n",
        "    df_temp = df_to_org[(df_to_org['userID'] == user )]\n",
        "    df_org = pd.concat([df_org,df_temp])\n",
        "\n",
        "  return df_org\n",
        "\n",
        "def orgMatDataframe(df_to_org,org_list):\n",
        "  \n",
        "  df_org = pd.DataFrame(index = list(df_to_org.index),columns = list(df_to_org.columns))\n",
        "  for user in org_list:\n",
        "    df_org.loc[user] = list(df_to_org.loc[user])\n",
        "  \n",
        "  return df_org\n",
        "\n",
        "def getItemsScore(userID,modelID,fileID):\n",
        "  \n",
        "  listeItemScore = pd.DataFrame()\n",
        "  \n",
        "  if modelID == 0:\n",
        "    listItemScore = predSeCF(userID,modelID,fileID)   #CF\n",
        "  elif modelID == 1:\n",
        "    listItemScore = predSeCF(userID,modelID,fileID)   #CF + Sentiment\n",
        "  elif modelID == 2:\n",
        "    listItemScore = predSSeCF(userID,modelID,fileID)  #CF + Social\n",
        "  elif modelID == 3:\n",
        "    listItemScore = predSSeCF(userID,modelID,fileID)  #CF + Sentiment + Social\n",
        "\n",
        "  return listeItemScore \n",
        "\n",
        "def getModel(modelID):\n",
        "  model = tf.keras.models.load_model(MODEL_PATH[modelID])\n",
        "  return model\n",
        "\n",
        "def loadDataFrame(df):\n",
        "  df_loaded = pd.read_csv(DATA[df])\n",
        "  return  df_loaded\n",
        "\n",
        "def predSeCF(userID,modelID,fileID):\n",
        "  \n",
        "  model = getModel(modelID)\n",
        "  df_user_item_mat = pd.read_csv(DATA[fileID])\n",
        "  SparseScoresVec = df_user_item_mat.loc[userID].to_numpy()\n",
        "  listItemScores = df_user_item_mat.loc[userID].to_frame()\n",
        "  listItemScores = listItemScores.reset_index()\n",
        "  listItemScores.set_axis(['itemID','score'],axis='columns',inplace=True)\n",
        "  PredScoresVec = model.predict(SparseScoresVec)\n",
        "  listItemScores.replace(SparseScoresVec,PredScoresVec,inplace = True)\n",
        "\n",
        "  return listItemScores\n",
        "\n",
        "def predSSeCF(userID,modelID,ratings_fileID,trust_fileID):\n",
        "\n",
        "  model = getModel(modelID)\n",
        "  df_user_item_mat = loadDataset(fileID)\n",
        "  df_user_user_mat = pd.read_csv(trust_fileID)\n",
        "  SparseScoresVec = df_user_item_mat.loc[userID].to_numpy()\n",
        "  SparseTrustVec = df_user_user_mat.loc[userID].to_numpy()\n",
        "  listItemScores = df_user_item_mat.loc[userID].to_frame()\n",
        "  listItemScores = listItemScores.reset_index()\n",
        "  listItemScores.set_axis(['itemID','score'],axis='columns',inplace=True)\n",
        "  PredScoresVec,SparseTrustVec = model.predict([SparseScoresVec,SparseTrustVec])\n",
        "  listItemScores.replace(SparseScoresVec,PredScoresVec,inplace = True)\n",
        "\n",
        "  return listItemScores\n",
        "  "
      ],
      "metadata": {
        "id": "z6KHUFF9gwT5"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def datapreprocess(df_sample):\n",
        "  \n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df_sample = generateIDs(df_sample,'userID','itemID',5000,6000+df_sample['userID'].nunique(),8000,8000+df_sample['itemID'].nunique())\n",
        "  \n",
        "  df_sample['one'] = df_sample['rating'].apply(lambda x: 1 if x==1 else 0)\n",
        "  df_sample['two'] = df_sample['rating'].apply(lambda x: 1 if x==2 else 0)\n",
        "  df_sample['three'] = df_sample['rating'].apply(lambda x: 1 if x==3 else 0)\n",
        "  df_sample['four'] = df_sample['rating'].apply(lambda x: 1 if x==4 else 0)\n",
        "  df_sample['five'] = df_sample['rating'].apply(lambda x: 1 if x==5 else 0)\n",
        "  #df['six'] = df['rating'].apply(lambda x: 1 if x==6 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df_sample"
      ],
      "metadata": {
        "id": "EHC86cO8GJcK"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = ['/content/review_shopping.csv.zip','/content/Yelp_shopping_GMF_filled_mat.csv.zip','/content/review_restaurant.csv.zip','/content/Yelp_restaurant_GMF_filled_mat.csv.zip']\n",
        "MODEL_PATH = []\n",
        "PATH = ''\n",
        "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "precision = tf.keras.metrics.Precision()\n",
        "METRICS = ['accuracy','mae',rmse]"
      ],
      "metadata": {
        "id": "3eS1ZsrzFU-m"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model,x_test,y_test = createAutoCF('S-AutoCF',2,3,50,tf.keras.losses.MeanSquaredError(),'rmsprop',0.3,2,8)"
      ],
      "metadata": {
        "id": "jWu3NfsAGS2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateModel(model,x_test,y_test)"
      ],
      "metadata": {
        "id": "b_zjz2jrR8Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = pd.read_csv(\"/content/gdrive/MyDrive/shopping_Beta_0.7.csv\")"
      ],
      "metadata": {
        "id": "YFf76MumHofs"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.rename(columns = {'text': 'reviewText', 'stars': 'rating', 'business_id': 'itemID', 'user_id': 'userID'}, inplace = True)\n",
        "df_sample.dropna(how='any',axis=0)\n",
        "df_sample.drop_duplicates(subset =['itemID', 'userID'] , keep = 'first' , inplace = True)"
      ],
      "metadata": {
        "id": "mvGZ7ZP-IGG-"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample.head()"
      ],
      "metadata": {
        "id": "ZnJVhwlYIluq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test = datapreprocess(df_sample)"
      ],
      "metadata": {
        "id": "c3PUFoU5GrhR",
        "outputId": "2a4e6a52-ba51-45da-a219-97b05c8d0156",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0% (21 of 2935) |                      | Elapsed Time: 0:00:00 ETA:   0:00:14"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== PREPROCESSING DATA ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100% (2935 of 2935) |####################| Elapsed Time: 0:00:18 Time:  0:00:18\n",
            "100% (9637 of 9637) |####################| Elapsed Time: 0:01:34 Time:  0:01:34\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== DATA PREPROCESSED ==\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8o-JzsJkgPQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['itemID'].max()"
      ],
      "metadata": {
        "id": "fPQ3C4zUJIgS",
        "outputId": "5fbe80f3-aa67-43eb-a0c3-bbd33df0acef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17636"
            ]
          },
          "metadata": {},
          "execution_count": 127
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df_test)"
      ],
      "metadata": {
        "id": "44G4_w6pgRAD",
        "outputId": "d95c3ace-6a07-40f1-d4a0-ce01eb621acb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genGMFmat(df_sample,df_test,'GMF',0,2,tf.keras.losses.MeanSquaredError(),'adam',8)"
      ],
      "metadata": {
        "id": "fZ9Cm1zW_lDY",
        "outputId": "777d2c61-a2f9-4837-ce2a-8d5b70b16962",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_39\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " user_ID (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " item_ID (InputLayer)           [(None, 1)]          0           []                               \n",
            "                                                                                                  \n",
            " user_emb_GMF (Embedding)       (None, 1, 8)         71472       ['user_ID[0][0]']                \n",
            "                                                                                                  \n",
            " item_emb_GMF (Embedding)       (None, 1, 8)         141096      ['item_ID[0][0]']                \n",
            "                                                                                                  \n",
            " flatten_78 (Flatten)           (None, 8)            0           ['user_emb_GMF[0][0]']           \n",
            "                                                                                                  \n",
            " flatten_79 (Flatten)           (None, 8)            0           ['item_emb_GMF[0][0]']           \n",
            "                                                                                                  \n",
            " multiply_39 (Multiply)         (None, 8)            0           ['flatten_78[0][0]',             \n",
            "                                                                  'flatten_79[0][0]']             \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 1)            9           ['multiply_39[0][0]']            \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 212,577\n",
            "Trainable params: 212,577\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/2\n",
            "4925/4925 [==============================] - 23s 4ms/step - loss: 5.8151 - accuracy: 0.0000e+00 - mae: 2.0977 - root_mean_squared_error: 2.1701 - val_loss: 1.0944 - val_accuracy: 0.0000e+00 - val_mae: 0.9054 - val_root_mean_squared_error: 1.0461\n",
            "Epoch 2/2\n",
            "4925/4925 [==============================] - 20s 4ms/step - loss: 0.9040 - accuracy: 0.0000e+00 - mae: 0.7563 - root_mean_squared_error: 0.9508 - val_loss: 0.8838 - val_accuracy: 0.0000e+00 - val_mae: 0.7393 - val_root_mean_squared_error: 0.9401\n",
            "358/358 [==============================] - 1s 2ms/step - loss: 0.9024 - accuracy: 0.0000e+00 - mae: 0.7499 - root_mean_squared_error: 0.9500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_s,item_s = FillSparseMat(model,df_test,df_sample)"
      ],
      "metadata": {
        "id": "kwAWuOmaABjb",
        "outputId": "7198a2c0-1e73-4741-8ee2-7fcb921b8970",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
            "100% (2935 of 2935) |####################| Elapsed Time: 0:08:14 Time:  0:08:14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled = Fill_Cf_Matrix(model,user_s,item_s,df_sample['userID'].unique(),df_sample['itemID'].unique())"
      ],
      "metadata": {
        "id": "nuALg9eIArRd",
        "outputId": "95a57cf3-cf74-47fa-9477-e482e9a5bef2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "366449/883894 [===========>..................] - ETA: 11:16"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_mat_filled.to_csv(\"/content/Yelp_shopping_GMF_filled_mat_07_BETA.csv\")"
      ],
      "metadata": {
        "id": "3b7mpg60dEsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"/content/Yelp_shopping_GMF_filled_mat_07_BETA.csv\")"
      ],
      "metadata": {
        "id": "5STcxr2W2AKg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}