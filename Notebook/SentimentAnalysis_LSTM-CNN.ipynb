{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download CellPhones -f CellPhonesRating.csv"
      ],
      "metadata": {
        "id": "UM0vifCk9CB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzC-uuMYYvXk"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check libcudnn8 version\n",
        "!apt-cache policy libcudnn8\n",
        "\n",
        "# Install latest version\n",
        "!apt install --allow-change-held-packages libcudnn8=8.4.1.50-1+cuda11.6\n",
        "\n",
        "# Export env variables\n",
        "!export PATH=/usr/local/cuda-11.4/bin${PATH:+:${PATH}}\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-11.4/lib64:$LD_LIBRARY_PATH\n",
        "!export LD_LIBRARY_PATH=/usr/local/cuda-11.4/include:$LD_LIBRARY_PATH\n",
        "!export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/extras/CUPTI/lib64"
      ],
      "metadata": {
        "id": "oHZP7bp73IiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LOYIL86oOtQo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import pandas as pd\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = ['/content/CellPhonesRating_50K_reviews.csv','dataset_2']\n",
        "MODEL_PATH = []\n",
        "PATH = ''\n",
        "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "precision = tf.keras.metrics.Precision()\n",
        "METRICS = ['accuracy','mae',rmse,precision]"
      ],
      "metadata": {
        "id": "JFLHXfiPuBjH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/CellPhonesRating.csv.zip')"
      ],
      "metadata": {
        "id": "b0JZpVf9vDXV"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = df[:50000]\n",
        "df_sample.to_csv('/content/CellPhonesRating_50K_reviews.csv')"
      ],
      "metadata": {
        "id": "-32p1FYAvheV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadBERT():\n",
        "  print(\"== LOADING BERT ...\")\n",
        "  bert_preprocess_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "  bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "  \n",
        "  print(\"== BERT LOADED ==\")\n",
        "  return bert_preprocess_model,bert_encoder\n",
        "\n",
        "def preproDataset(df):\n",
        "  \n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.drop_duplicates(subset =['productID', 'reviewerID'] , keep = 'first' , inplace = True)\n",
        "\n",
        "  df['one']=df['rating'].apply(lambda x: 1 if x==1.0 else 0)\n",
        "  df['two']=df['rating'].apply(lambda x: 1 if x==2.0 else 0)\n",
        "  df['three']=df['rating'].apply(lambda x: 1 if x==3.0 else 0)\n",
        "  df['four']=df['rating'].apply(lambda x: 1 if x==4.0 else 0)\n",
        "  df['five']=df['rating'].apply(lambda x: 1 if x==5.0 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def getBLCNNmodel(emb_size,filter):\n",
        "\n",
        "  bert_preprocess_model,bert_encoder = loadBERT()\n",
        "  #BERT layers\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessed_review = bert_preprocess_model(text_input)\n",
        "  outputs = bert_encoder(preprocessed_review)\n",
        "\n",
        "  # LSTM + CNN layers\n",
        "\n",
        "  l = tf.keras.layers.LSTM(emb_size, dropout = 0.1, return_sequences=True)(outputs['sequence_output'])\n",
        "\n",
        "  conv_1 = tf.keras.layers.Conv1D(filters=filter, kernel_size=(3), activation='relu')(l)\n",
        "  pool_1 = tf.keras.layers.MaxPooling1D((3))(conv_1)\n",
        "        \n",
        "  flatten = layers.Flatten()(pool_1)\n",
        "  hidden1 = layers.Dense(64, activation='relu')(flatten)\n",
        "  output = layers.Dense(5, activation='softmax')(hidden1)\n",
        "\n",
        "  model = tf.keras.Model(inputs = text_input, outputs = output)\n",
        "\n",
        "  return model\n",
        "\n",
        "def createSentModel(modelID,fileID,nbrE,lossF,OF,emb,filter,cb = None):\n",
        "\n",
        "  sparseDf = loadDataset(fileID)\n",
        "  sparseDf = sparseDf\n",
        "  sparseDf = preproDataset(sparseDf)\n",
        "  x_train, x_test, y_train, y_test = train_test_split(sparseDf['reviewText'],sparseDf[['one','two','three','four','five']],test_size = 0.2,stratify=sparseDf[['one','two','three','four','five']])\n",
        "  model_trained = trainModel(modelID,nbrE,lossF,OF,x_train,y_train,embed_size = emb,filter_size = filter,cb = cb)\n",
        "\n",
        "  return model_trained,x_test,y_test\n",
        "\n",
        "def evaluateModel(model,x_test,y_test):\n",
        "  model.evaluate(x_test,y_test)\n",
        "\n",
        "def loadDataset(fileID):\n",
        "  dataset = pd.read_csv(DATA[fileID])\n",
        "  print(\"== FILE LOADED ==\")\n",
        "  return dataset\n",
        "\n",
        "def trainModel(modelID,nbrEpochs,lossF,OF,x_train ,y_train ,mid_layer_ratio=None,nb_layers=None,maxUserID = None,maxItemID = None,embed_size = None,filter_size = None, cb = None):\n",
        "  \n",
        "  if modelID =='BLCNN':\n",
        "    model = getBLCNNmodel(embed_size,filter_size)\n",
        "  if modelID == 'GMF':\n",
        "    model = getGMFmodel(maxUserID,maxItemID,embed_size)\n",
        "  elif modelID == 'S-AutoCF':\n",
        "    model = getAutoCFmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "  elif modelID == 'SS-AutoCF':\n",
        "    model = getSS_HAEmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "\n",
        "  model.compile(optimizer = OF,\n",
        "                    loss = lossF,\n",
        "                    metrics= METRICS)\n",
        "  print(\"== TRAINING IN PROGRESS ...\")\n",
        "  model.fit(x_train,y_train,epochs = nbrEpochs,callbacks =[cb])\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "mmxBz3Ilee7b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cHzYcIcgtKbV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1cb9732-be7a-4fb5-9d23-5bf829bb27e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        " \n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating checkpoint directory to save model's weights\n",
        "checkpoint_path = \"/content/gdrive/MyDrive/training_LSTM_CNN_Full_CP_Dataset/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "metadata": {
        "id": "aF0qvLb6-bjx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trained,x_test,y_test = createSentModel('BLCNN',0,20,tf.keras.losses.CategoricalCrossentropy(),'adam',600,64, cp_callback)"
      ],
      "metadata": {
        "id": "W60sdySWuaq3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff40debe-f9a1-4131-a89f-60b490663835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "== FILE LOADED ==\n",
            "== PREPROCESSING DATA ...\n",
            "== DATA PREPROCESSED ==\n",
            "== LOADING BERT ...\n",
            "== BERT LOADED ==\n",
            "== TRAINING IN PROGRESS ...\n",
            "Epoch 1/20\n",
            "1244/1244 [==============================] - ETA: 0s - loss: 0.9799 - accuracy: 0.6097 - mae: 0.2034 - root_mean_squared_error: 0.3198 - precision: 0.7428\n",
            "Epoch 1: saving model to /content/gdrive/MyDrive/training_LSTM_CNN_Full_CP_Dataset/cp.ckpt\n",
            "1244/1244 [==============================] - 469s 366ms/step - loss: 0.9799 - accuracy: 0.6097 - mae: 0.2034 - root_mean_squared_error: 0.3198 - precision: 0.7428\n",
            "Epoch 2/20\n",
            " 239/1244 [====>.........................] - ETA: 6:06 - loss: 0.8862 - accuracy: 0.6391 - mae: 0.1889 - root_mean_squared_error: 0.3076 - precision: 0.7564"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "evaluateModel(model_trained,x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNMW-ekU42sU",
        "outputId": "ef987e97-c6a4-4c72-fac3-02618878c4e3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "311/311 [==============================] - 109s 347ms/step - loss: 0.8800 - accuracy: 0.6481 - mae: 0.1814 - root_mean_squared_error: 0.3050 - precision_1: 0.7481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_trained.save('/content/gdrive/MyDrive/model_LSTM_CNN_E5-EM200-F128')"
      ],
      "metadata": {
        "id": "jwEjy_WBmr7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Isc-ZQ9lHa4"
      },
      "outputs": [],
      "source": [
        "bert_cnn_model.fit(X_train,y_train,epochs = 25,callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_cnn_model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "r9fLRLCpBa5g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SentimentAnalysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
