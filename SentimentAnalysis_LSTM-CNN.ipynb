{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download CellPhones -f CellPhonesRating.csv"
      ],
      "metadata": {
        "id": "UM0vifCk9CB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzC-uuMYYvXk"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LOYIL86oOtQo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import pandas as pd\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA = ['/content/CellPhonesRating_50K_reviews.csv','yelp_GMF_rating_mat','yelp_rating_mat','yelp_user_mat']\n",
        "MODEL_PATH = []\n",
        "PATH = ''\n",
        "rmse = tf.keras.metrics.RootMeanSquaredError()\n",
        "precision = tf.keras.metrics.Precision()\n",
        "METRICS = ['accuracy','mae',rmse,precision]"
      ],
      "metadata": {
        "id": "JFLHXfiPuBjH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/CellPhonesRating.csv.zip')"
      ],
      "metadata": {
        "id": "b0JZpVf9vDXV"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sample = df[:50000]\n",
        "df_sample.to_csv('/content/CellPhonesRating_50K_reviews.csv')"
      ],
      "metadata": {
        "id": "-32p1FYAvheV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loadBERT():\n",
        "  print(\"== LOADING BERT ...\")\n",
        "  bert_preprocess_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "  bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "  \n",
        "  print(\"== BERT LOADED ==\")\n",
        "  return bert_preprocess_model,bert_encoder\n",
        "\n",
        "def preproDataset(df):\n",
        "  \n",
        "  print(\"== PREPROCESSING DATA ...\")\n",
        "  df = df.dropna(how='any',axis=0)\n",
        "  df.drop_duplicates(subset =['productID', 'reviewerID'] , keep = 'first' , inplace = True)\n",
        "\n",
        "  df['one']=df['rating'].apply(lambda x: 1 if x==1.0 else 0)\n",
        "  df['two']=df['rating'].apply(lambda x: 1 if x==2.0 else 0)\n",
        "  df['three']=df['rating'].apply(lambda x: 1 if x==3.0 else 0)\n",
        "  df['four']=df['rating'].apply(lambda x: 1 if x==4.0 else 0)\n",
        "  df['five']=df['rating'].apply(lambda x: 1 if x==5.0 else 0)\n",
        "  print(\"== DATA PREPROCESSED ==\")\n",
        "\n",
        "  return df\n",
        "\n",
        "def getBLCNNmodel(emb_size,filter):\n",
        "\n",
        "  bert_preprocess_model,bert_encoder = loadBERT()\n",
        "  #BERT layers\n",
        "  text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
        "  preprocessed_review = bert_preprocess_model(text_input)\n",
        "  outputs = bert_encoder(preprocessed_review)\n",
        "\n",
        "  # LSTM + CNN layers\n",
        "\n",
        "  l = tf.keras.layers.LSTM(emb_size, dropout = 0.1, return_sequences=True)(outputs['sequence_output'])\n",
        "\n",
        "  conv_1 = tf.keras.layers.Conv1D(filters=filter, kernel_size=(3), activation='relu')(l)\n",
        "  pool_1 = tf.keras.layers.MaxPooling1D((3))(conv_1)\n",
        "        \n",
        "  flatten = layers.Flatten()(pool_1)\n",
        "  hidden1 = layers.Dense(64, activation='relu')(flatten)\n",
        "  output = layers.Dense(5, activation='softmax')(hidden1)\n",
        "\n",
        "  model = tf.keras.Model(inputs = text_input, outputs = output)\n",
        "\n",
        "  return model\n",
        "\n",
        "def createSentModel(modelID,fileID,nbrE,lossF,OF,emb,filter):\n",
        "\n",
        "  sparseDf = loadDataset(fileID)\n",
        "  sparseDf = sparseDf[:2000]\n",
        "  sparseDf = preproDataset(sparseDf)\n",
        "  x_train, x_test, y_train, y_test = train_test_split(sparseDf['reviewText'],sparseDf[['one','two','three','four','five']], stratify=sparseDf[['one','two','three','four','five']])\n",
        "  model_trained = trainModel(modelID,nbrE,lossF,OF,x_train,y_train,embed_size = emb,filter_size = filter)\n",
        "\n",
        "  return model_trained,x_test,y_test\n",
        "\n",
        "def evaluateModel(model,x_test,y_test):\n",
        "  model.evaluate(x_test,y_test)\n",
        "\n",
        "def loadDataset(fileID):\n",
        "  dataset = pd.read_csv(DATA[fileID])\n",
        "  print(\"== FILE LOADED ==\")\n",
        "  return dataset\n",
        "\n",
        "def trainModel(modelID,nbrEpochs,lossF,OF,x_train ,y_train ,mid_layer_ratio=None,nb_layers=None,maxUserID = None,maxItemID = None,embed_size = None,filter_size = None):\n",
        "  \n",
        "  if modelID =='BLCNN':\n",
        "    model = getBLCNNmodel(embed_size,filter_size)\n",
        "    print(model.summary())\n",
        "  if modelID == 'GMF':\n",
        "    model = getGMFmodel(maxUserID,maxItemID,embed_size)\n",
        "  elif modelID == 'S-AutoCF':\n",
        "    model = getAutoCFmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "  elif modelID == 'SS-AutoCF':\n",
        "    model = getSS_HAEmodel(x_train,mid_layer_ratio,nb_layers)\n",
        "\n",
        "  model.compile(optimizer = OF,\n",
        "                    loss = lossF,\n",
        "                    metrics= METRICS)\n",
        "  print(\"== TRAINING IN PROGRESS ...\")\n",
        "  model.fit(x_train,y_train,epochs = nbrEpochs)\n",
        "  \n",
        "  return model"
      ],
      "metadata": {
        "id": "mmxBz3Ilee7b"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHzYcIcgtKbV"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        " \n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating checkpoint directory to save model's weights\n",
        "checkpoint_path = \"/content/gdrive/MyDrive/training_LSTM_CNN_Full_CP_Dataset/cp.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)"
      ],
      "metadata": {
        "id": "aF0qvLb6-bjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\""
      ],
      "metadata": {
        "id": "5ULz300c1AmH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_trained,x_test,y_test = createSentModel('BLCNN',0,25,tf.keras.losses.CategoricalCrossentropy(),'adam',200,64)"
      ],
      "metadata": {
        "id": "W60sdySWuaq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-gpu"
      ],
      "metadata": {
        "id": "PuofsMtv6PSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "id": "WUxcrNRq6kfp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.experimental.set_memory_growth(gpus[0], True)"
      ],
      "metadata": {
        "id": "vKb4UtB36pEJ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "pakrg_yB6B5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Isc-ZQ9lHa4"
      },
      "outputs": [],
      "source": [
        "bert_cnn_model.fit(X_train,y_train,epochs = 25,callbacks=[cp_callback])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bert_cnn_model.load_weights(checkpoint_path)"
      ],
      "metadata": {
        "id": "r9fLRLCpBa5g"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SentimentAnalysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}